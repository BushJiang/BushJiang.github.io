<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>写给新读者的导航</title>
    <link href="/2025/01/07/%E5%86%99%E7%BB%99%E6%96%B0%E8%AF%BB%E8%80%85%E7%9A%84%E5%AF%BC%E8%88%AA/"/>
    <url>/2025/01/07/%E5%86%99%E7%BB%99%E6%96%B0%E8%AF%BB%E8%80%85%E7%9A%84%E5%AF%BC%E8%88%AA/</url>
    
    <content type="html"><![CDATA[<p>你好啊朋友，我是江浩，一名AI大陆的探险者，目前主要关注向量数据库和大语言模型领域。</p><p>在博客里，我会探秘AI神奇能力的背后原理。别担心，我会用有趣的言语和生动的类比来解释这些原理。你是否好奇，孙悟空 + 红楼梦 - 西游记 &#x3D; ？那就来了解下向量嵌入吧。当你接触到向量嵌入后，你可能会问，既然已经有了稠密向量，为什么还需要稀疏向量？嗯，如果说稠密向量是领域专家，那么稀疏向量就是一个聪明的门外汉，面对领域外的知识，请教后者反而更合适。</p><p>除了原理探秘，我还会用AI开发一些有趣的应用，比如，用白话文搜索语义相似的古诗词，让你体验一把“文艺青年”的乐趣。或者，开发一个“鲁迅说没有”的RAG应用，验证把所谓的“鲁迅名言”是否属实，它的原文又是怎样的。甚至，我还想和牛魔王对话，问问他更爱铁扇公主还是玉面狐狸。哈哈，有趣的想法太多，慢慢实现。</p><p>对了，我在探索AI大陆时，也采集了不少鲜美的果实————优质资源，我会整理好了分享给你。</p><p>AI大陆有趣又神奇，朋友，我邀请你和我同行。</p><p>ChangeLog<br>2025-01-07</p>]]></content>
    
    
    <categories>
      
      <category>杂谈</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>孙悟空 + 红楼梦 - 西游记 = ？向量嵌入之稠密向量</title>
    <link href="/2024/10/11/%E5%AD%99%E6%82%9F%E7%A9%BA-%E7%BA%A2%E6%A5%BC%E6%A2%A6-%E8%A5%BF%E6%B8%B8%E8%AE%B0-%EF%BC%9F%E5%90%91%E9%87%8F%E5%B5%8C%E5%85%A5%E4%B9%8B%E7%A8%A0%E5%AF%86%E5%90%91%E9%87%8F/"/>
    <url>/2024/10/11/%E5%AD%99%E6%82%9F%E7%A9%BA-%E7%BA%A2%E6%A5%BC%E6%A2%A6-%E8%A5%BF%E6%B8%B8%E8%AE%B0-%EF%BC%9F%E5%90%91%E9%87%8F%E5%B5%8C%E5%85%A5%E4%B9%8B%E7%A8%A0%E5%AF%86%E5%90%91%E9%87%8F/</url>
    
    <content type="html"><![CDATA[<p>一起来开个脑洞，如果孙悟空穿越到红楼梦的世界，他会成为谁？贾宝玉，林黛玉，还是薛宝钗？这看似一道文学题，但是我们不妨用数学方法来求解：<code>孙悟空 + 红楼梦 - 西游记 = ？</code></p><p>文字也能做运算？当然不行，但是把文字转换成数字之后，就可以用来计算了。而这个过程，叫做 “向量嵌入”。为什么要做向量嵌入？因为具有语义意义的数据，比如文本或者图像，人可以分辨相关程度，但是无法量化，更不能计算。比如，对于一组词“孙悟空、猪八戒、沙僧、西瓜、苹果、香蕉“，我会把“孙悟空、猪八戒、沙僧”分成一组，“西瓜、苹果、香蕉”分成另一组。但是，如果进一步提问，“孙悟空”是和“猪八戒”更相关，还是和“沙僧”更相关呢？这很难回答。</p><p>而把这些信息转换成向量后，相关程度就可以通过它们在向量空间中的距离量化。甚至于，我们可以做 <code>孙悟空 + 红楼梦 - 西游记 = ？</code> 这样的脑洞数学题。</p><p>本文首发于 Zilliz 公众号。文中代码的 Notebook 在<a href="https://pan.baidu.com/s/1VtPt-6Y_hhxKn9uB4AMNFg?pwd=7zv9">这里</a>下载。</p><h2 id="文字是怎么变成向量的"><a href="#文字是怎么变成向量的" class="headerlink" title="文字是怎么变成向量的"></a>文字是怎么变成向量的</h2><p>怎么把文字变成向量呢？首先出现的是词向量，其中的代表是 word2vec 模型。它先准备一张词汇表，给每个词随机赋予一个向量，然后利用大量语料，通过 CBOW（Continuous Bag-of-Words）和 Skip-Gram 两种方法训练模型，不断优化字词的向量。</p><p>CBOW 使用上下文（周围的词）预测目标词<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="严格来说，“目标词”不是单词而是“token”。token 是组成句子的基本单元。对于英文来说，token可以简单理解为单词，还可能是子词（subword）或者标点符号，比如“unhappiness” 可能会被分割成“un”和“happiness“。对于汉字来说，则是字、词或者短语，汉字不会像英文单词那样被分割。">[1]</span></a></sup>，而 Skip-Gram 则相反，通过目标词预测它的上下文。举个例子，对于“我爱吃冰淇淋”这个句子，CBOW方法已知上下文“我爱“和”冰淇淋”，计算出中间词的概率，比如，“吃”的概率是90%，“喝”的概率是7%，“玩”的概率是3%。然后再使用损失函数预测概率与实际概率的差异，最后通过反向传播算法，调整词向量模型的参数，使得损失函数最小化。训练词向量模型的最终目的，是捕捉词汇之间的语义关系，使得相关的词在向量空间中距离更近。</p><p>打个比方，最初的词向量模型就像一个刚出生的孩子，对字词的理解是模糊的。父母在各种场景下和孩子说话，时不时考一考孩子，相当于用语料库训练模型。只不过训练模型的过程是不断迭代神经网络的参数，而教孩子说话，则是改变大脑皮层中神经元突触的连接。</p><p>比如，父母会在吃饭前跟孩子说：<br>“肚子饿了就要…”<br>“要吃饭。”</p><p>如果答错了，父母会纠正孩子：<br>“吃饭之前要…”<br>“要喝汤。”<br>“不对，吃饭之前要洗手。”</p><p>这就是在调整模型的参数。</p><p>好了，纸上谈兵结束，咱们用代码实际操练一番吧。</p><p>版本说明：<br>Milvus 版本：&gt;&#x3D;2.5.0<br>pymilvus 版本：&gt;&#x3D;2.5.0</p><p>安装依赖：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip install gensim scikit-learn transformers matplotlib<br></code></pre></td></tr></table></figure><p>从 gensim.models 模块中导入 KeyedVectors 类，它用于存储和操作词向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> gensim.models <span class="hljs-keyword">import</span> KeyedVectors<br></code></pre></td></tr></table></figure><p>在<a href="https://github.com/Embedding/Chinese-Word-Vectors/blob/master/README_zh.md">这里</a>下载中文词向量模型 <code>Literature 文学作品</code>，并且加载该模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 加载中文词向量模型</span><br>word_vectors = KeyedVectors.load_word2vec_format(<span class="hljs-string">&#x27;sgns.literature.word&#x27;</span>, binary=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>词向量模型其实就像一本字典。在字典里，每个字对应的是一条解释，在词向量模型中，每个词对应的是一个向量。</p><p>我们使用的词向量模型是300维的，数量太多，可以只显示前4个维度的数值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;孙悟空&#x27;的向量的前四个维度：<span class="hljs-subst">&#123;word_vectors[<span class="hljs-string">&#x27;孙悟空&#x27;</span>].tolist()[:<span class="hljs-number">4</span>]&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">&#x27;孙悟空&#x27;的向量的前四个维度：[-0.09262000024318695, -0.034056998789310455, -0.16306699812412262, -0.05771299824118614]<br></code></pre></td></tr></table></figure><h2 id="语义更近，距离更近"><a href="#语义更近，距离更近" class="headerlink" title="语义更近，距离更近"></a>语义更近，距离更近</h2><p>前面我们提出了疑问，“孙悟空”是和“猪八戒”更相关，还是和“沙僧”更相关呢？在 [[01-如何假装文艺青年，怎么把大白话“变成”古诗词？]] 这篇文章中，我们使用内积 <code>IP</code> 计算两个向量的距离，这里我们使用余弦相似度来计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;孙悟空&#x27;和&#x27;猪八戒&#x27;向量的余弦相似度是：<span class="hljs-subst">&#123;word_vectors.similarity(<span class="hljs-string">&#x27;孙悟空&#x27;</span>, <span class="hljs-string">&#x27;猪八戒&#x27;</span>):<span class="hljs-number">.2</span>f&#125;</span>&quot;</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;孙悟空&#x27;和&#x27;沙僧&#x27;向量的余弦相似度是：<span class="hljs-subst">&#123;word_vectors.similarity(<span class="hljs-string">&#x27;孙悟空&#x27;</span>, <span class="hljs-string">&#x27;沙僧&#x27;</span>):<span class="hljs-number">.2</span>f&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>返回：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">&#x27;孙悟空&#x27;和&#x27;猪八戒&#x27;向量的余弦相似度是：0.60<br>&#x27;孙悟空&#x27;和&#x27;沙僧&#x27;向量的余弦相似度是：0.59<br></code></pre></td></tr></table></figure><p>看来，孙悟空还是和猪八戒更相关。但是我们还不满足，我们还想知道，和孙悟空最相关的是谁。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 查找与“孙悟空”最相关的4个词</span><br>similar_words = word_vectors.most_similar(<span class="hljs-string">&quot;孙悟空&quot;</span>, topn=<span class="hljs-number">4</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;与&#x27;孙悟空&#x27;最相关的4个词分别是：&quot;</span>)<br><span class="hljs-keyword">for</span> word, similarity <span class="hljs-keyword">in</span> similar_words:<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;word&#125;</span>， 余弦相似度为：<span class="hljs-subst">&#123;similarity:<span class="hljs-number">.2</span>f&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>返回：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">与&#x27;孙悟空&#x27;最相关的4个词分别是：<br>悟空， 余弦相似度为：0.66<br>唐僧， 余弦相似度为：0.61<br>美猴王， 余弦相似度为：0.61<br>猪八戒， 余弦相似度为：0.60<br></code></pre></td></tr></table></figure><p>“孙悟空”和“悟空”、“美猴王”相关，这容易理解。为什么它还和“唐僧”、“猪八戒”相关呢？前面提到的词向量模型的训练原理解释，就是因为在训练文本中，“唐僧”、“猪八戒”经常出现在“孙悟空”这个词的上下文中。这不难理解——在《西游记》中，孙悟空经常救唐僧，还喜欢戏耍八戒。</p><p>前面提到，训练词向量模型是为了让语义相关的词，在向量空间中距离更近。那么，我们可以测试一下，给出四组语义相近的词，考一考词向量模型，看它能否识别出来。</p><p>第一组：西游记，三国演义，水浒传，红楼梦<br>第二组：西瓜，苹果，香蕉，梨<br>第三组：长江，黄河</p><p>首先，获取这四组词的词向量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入用于数值计算的库</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 定义要可视化的单词列表</span><br>words = [<span class="hljs-string">&quot;西游记&quot;</span>, <span class="hljs-string">&quot;三国演义&quot;</span>, <span class="hljs-string">&quot;水浒传&quot;</span>, <span class="hljs-string">&quot;红楼梦&quot;</span>, <br>        <span class="hljs-string">&quot;西瓜&quot;</span>, <span class="hljs-string">&quot;苹果&quot;</span>, <span class="hljs-string">&quot;香蕉&quot;</span>, <span class="hljs-string">&quot;梨&quot;</span>, <br>        <span class="hljs-string">&quot;长江&quot;</span>, <span class="hljs-string">&quot;黄河&quot;</span>]<br><br><span class="hljs-comment"># 使用列表推导式获取每个单词的向量</span><br>vectors = np.array([word_vectors[word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words])<br></code></pre></td></tr></table></figure><p>然后，使用 PCA （Principal Component Analysis，组成分分析）把200维的向量降到2维，一个维度作为 x 坐标，另一个维度作为 y 坐标，这样就把高维向量投影到平面了，方便我们在二维图形上显示它们。换句话说，PCA 相当于《三体》中的二向箔，对高维向量实施了降维打击。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入用于降维的PCA类</span><br><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<br><br><span class="hljs-comment"># 创建PCA对象，设置降至2维</span><br>pca = PCA(n_components=<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># 对词向量实施PCA降维</span><br>vectors_pca = pca.fit_transform(vectors)<br></code></pre></td></tr></table></figure><p>最后，在二维图形上显示降维后的向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入用于绘图的库</span><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-comment"># 创建一个5x5英寸的图</span><br>fig, axes = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, figsize=(<span class="hljs-number">7</span>, <span class="hljs-number">7</span>))<br><br><span class="hljs-comment"># 设置中文字体</span><br>plt.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>] = [<span class="hljs-string">&#x27;Heiti TC&#x27;</span>]<br><span class="hljs-comment"># 确保负号能够正确显示</span><br>plt.rcParams[<span class="hljs-string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="hljs-literal">False</span>  <br><br><span class="hljs-comment"># 使用PCA降维后的前两个维度作为x和y坐标绘制散点图</span><br>axes.scatter(vectors_pca[:, <span class="hljs-number">0</span>], vectors_pca[:, <span class="hljs-number">1</span>])<br><br><span class="hljs-comment"># 为每个点添加文本标注</span><br><span class="hljs-keyword">for</span> i, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(words):<br>    <span class="hljs-comment"># 添加注释，设置文本内容、位置、样式等</span><br>    <span class="hljs-comment"># 要显示的文本（单词）</span><br>    axes.annotate(word,<br>                  <span class="hljs-comment"># 点的坐标</span><br>                  (vectors_pca[i, <span class="hljs-number">0</span>], vectors_pca[i, <span class="hljs-number">1</span>]),  <br>                  <span class="hljs-comment"># 文本相对于点的偏移量</span><br>                  xytext=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>),  <br>                  <span class="hljs-comment"># 指定偏移量的单位</span><br>                  textcoords=<span class="hljs-string">&#x27;offset points&#x27;</span>,  <br>                  <span class="hljs-comment"># 字体大小</span><br>                  fontsize=<span class="hljs-number">10</span>,  <br>                  <span class="hljs-comment"># 字体粗细</span><br>                  fontweight=<span class="hljs-string">&#x27;bold&#x27;</span>)  <br><br><span class="hljs-comment"># 设置图表标题和字体大小</span><br>axes.set_title(<span class="hljs-string">&#x27;词向量&#x27;</span>, fontsize=<span class="hljs-number">14</span>)<br><br><span class="hljs-comment"># 自动调整子图参数，使之填充整个图像区域</span><br>plt.tight_layout()<br><br><span class="hljs-comment"># 在屏幕上显示图表</span><br>plt.show()<br></code></pre></td></tr></table></figure><p>从图中可以看出，同一组词的确在图中的距离更近。</p><p><img src="https://picgo233.oss-cn-hangzhou.aliyuncs.com/img/202409161007773.png" alt="600"></p><p>既然可以把高维向量投影到二维，那么是不是也能投影到三维呢？当然可以，那样更酷。你可以在 <a href="https://projector.tensorflow.org/">TensorFlow Embedding Projector</a> 上尝试下，输入单词，搜索与它最近的几个词，看看它们在三维空间上的位置关系。</p><p>比如，输入 <code>apple</code>，最接近的5个词分别是 <code>OS</code>、<code>macintosh</code>、<code>amiga</code>、<code>ibm</code> 和 <code>microsoft</code>。</p><p><img src="https://picgo233.oss-cn-hangzhou.aliyuncs.com/img/202409161023693.png" alt="600"></p><h2 id="如果孙悟空穿越到红楼梦"><a href="#如果孙悟空穿越到红楼梦" class="headerlink" title="如果孙悟空穿越到红楼梦"></a>如果孙悟空穿越到红楼梦</h2><p>回到我们开篇的问题，把文本向量化后，就可以做运算了。如果孙悟空穿越到红楼梦，我们用下面的数学公式表示：<br><code>孙悟空 + 红楼梦 - 西游记</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">result = word_vectors.most_similar(positive=[<span class="hljs-string">&quot;孙悟空&quot;</span>, <span class="hljs-string">&quot;红楼梦&quot;</span>], negative=[<span class="hljs-string">&quot;西游记&quot;</span>], topn=<span class="hljs-number">4</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;孙悟空 + 红楼梦 - 西游记 = <span class="hljs-subst">&#123;result&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>答案为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">孙悟空 + 红楼梦 - 西游记 = [(&#x27;唐僧&#x27;, 0.4163001477718353), (&#x27;贾宝玉&#x27;, 0.41606390476226807), (&#x27;妙玉&#x27;, 0.39432790875434875), (&#x27;沙和尚&#x27;, 0.3922004997730255)]<br></code></pre></td></tr></table></figure><p>你是不是有点惊讶，因为答案中的“唐僧”和“沙和尚”根本就不是《红楼梦》中的人物。这是因为虽然词向量可以反映字词之间的语义相关性，但是它终究是在做数学题，不能像人类一样理解“孙悟空 + 红楼梦 - 西游记”背后的含义。答案中出现“唐僧”和“沙和尚”是因为它们和“孙悟空”更相关，而出现“贾宝玉”和“妙玉”则是因为它们和“红楼梦”更相关。</p><p>不过，这样的测试还蛮有趣的，你也可以多尝试一下，有的结果还蛮符合直觉的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">result = word_vectors.most_similar(positive=[<span class="hljs-string">&quot;牛奶&quot;</span>, <span class="hljs-string">&quot;发酵&quot;</span>], topn=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;牛奶 + 发酵 = <span class="hljs-subst">&#123;result[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]&#125;</span>&quot;</span>)<br><br>result = word_vectors.most_similar(positive=[<span class="hljs-string">&quot;男人&quot;</span>, <span class="hljs-string">&quot;泰国&quot;</span>], topn=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;男人 + 泰国 = <span class="hljs-subst">&#123;result[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>计算的结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">牛奶 + 发酵 = 变酸<br>男人 + 泰国 = 女人<br></code></pre></td></tr></table></figure><h2 id="待优化！"><a href="#待优化！" class="headerlink" title="待优化！"></a>待优化！</h2><p>尝试把上面的计算题降维，显示在图像上，看看是否满足两个向量相加，等于第三个向量</p><h2 id="一词多义怎么办"><a href="#一词多义怎么办" class="headerlink" title="一词多义怎么办"></a>一词多义怎么办</h2><p>前面说过，词向量模型就像一本字典，每个词对应一个向量，而且是唯一一个向量。但是，在语言中一词多义的现象是非常常见的，比如对于 “苹果” 这个词，既可以指一种水果，也可以指一家电子产品公司。词向量模型在训练 “苹果”这个词的向量时，这两种语义都会考虑到，所以它在向量空间中将位于“水果”和 “电子产品公司”之间。这就好像你3月20号过生日，你同事3月30号过生日，你的领导为了给你们两个人一起过庆祝生日，选择了3月25号——不是任何一个人的生日。</p><p>为了解决一词多义的问题，BERT（Bidirectional Encoder Representations from Transformers）模型诞生了。它是一种基于深度神经网络的预训练语言模型，使用 Transformer 架构，通过自注意力机制同时考虑一个 token 的前后上下文，并且根据上下文环境更新该 token 的向量。</p><p>比如，“苹果”这个目标词的初始向量是从词库中获取的，向量的值是固定的。当注意力模型处理“苹果“这个词时，如果发现上下文中有“手机”一词，会给它分配更多权重，“苹果”的向量会更新，靠近“手机”的方向。如果上下文中有“水果”一词，则会靠近“水果”的方向。</p><p>注意力模型分配权重是有策略的。它只会给上下文中与目标词关系紧密的词分配更多权重。所以，BERT 能够理解目标词与上下文之间的语义关系，根据上下文调整目标词的向量。</p><p>BERT 的预训练分成两种训练方式。第一种训练方式叫做“掩码语言模型（Masked Language Model，MLM）”，和 word2vec 相似，它会随机选择句子中的一些词遮住，根据上下文信息预测这个词，再根据预测结果与真实结果的差异调整参数。第二种训练方式叫做“下一句预测（Next Sentence Prediction，NSP）”，每次输入两个句子，判断第二个句子是否是第一个句子的下一句，然后同样根据结果差异调整参数。</p><p>说了这么多，BERT 模型的效果究竟怎么样？让我们动手试试吧。首先导入 BERT 模型，定义一个获取句子中指定单词的向量的函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 从transformers库中导入BertTokenizer类和BertModel类</span><br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, BertModel<br><br><span class="hljs-comment"># 加载分词器 BertTokenizer</span><br>bert_tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&#x27;bert-base-chinese&#x27;</span>)<br><br><span class="hljs-comment"># 加载嵌入模型 BertModel</span><br>bert_model = BertModel.from_pretrained(<span class="hljs-string">&#x27;bert-base-chinese&#x27;</span>)<br><br><span class="hljs-comment"># 使用BERT获取句子中指定单词的向量</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_bert_emb</span>(<span class="hljs-params">sentence, word</span>):<br>    <span class="hljs-comment"># 使用 bert_tokenizer 对句子编码</span><br>    <span class="hljs-built_in">input</span> = bert_tokenizer(sentence, return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>)<br>    <span class="hljs-comment"># 将编码传递给 BERT 模型，计算所有层的输出</span><br>    output = bert_model(**<span class="hljs-built_in">input</span>)<br>    <span class="hljs-comment"># 获取 BERT 模型最后一层的隐藏状态，它包含了每个单词的嵌入信息</span><br>    last_hidden_states = output.last_hidden_state<br>    <span class="hljs-comment"># 将输入的句子拆分成单词，并生成一个列表</span><br>    word_tokens = bert_tokenizer.tokenize(sentence)<br>    <span class="hljs-comment"># 获取目标单词在列表中的索引位置</span><br>    word_index = word_tokens.index(word)<br>    <span class="hljs-comment"># 从最后一层隐藏状态中提取目标单词的嵌入表示</span><br>    word_emb = last_hidden_states[<span class="hljs-number">0</span>, word_index + <span class="hljs-number">1</span>, :]<br>    <span class="hljs-comment"># 返回目标单词的嵌入表示</span><br>    <span class="hljs-keyword">return</span> word_emb<br></code></pre></td></tr></table></figure><p>然后通过 BERT 和词向量模型分别获取两个句子中指定单词的向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">sentence1 = <span class="hljs-string">&quot;我今天很开心。&quot;</span><br>sentence2 = <span class="hljs-string">&quot;我打开了房门。&quot;</span><br>word = <span class="hljs-string">&quot;开&quot;</span><br><br><span class="hljs-comment"># 使用 BERT 模型获取句子中指定单词的向量</span><br>bert_emb1 = get_bert_emb(sentence1, word).detach().numpy()<br><br>bert_emb2 = get_bert_emb(sentence2, word).detach().numpy()<br><br><span class="hljs-comment"># 使用词向量模型获取指定单词的向量</span><br>word_emb = word_vectors[word]<br></code></pre></td></tr></table></figure><p>最后，查看这三个向量的区别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;在句子 &#x27;<span class="hljs-subst">&#123;sentence1&#125;</span>&#x27; 中，&#x27;<span class="hljs-subst">&#123;word&#125;</span>&#x27;的向量的前四个维度：<span class="hljs-subst">&#123;bert_emb1[: <span class="hljs-number">4</span>]&#125;</span>&quot;</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;在句子 &#x27;<span class="hljs-subst">&#123;sentence2&#125;</span>&#x27; 中，&#x27;<span class="hljs-subst">&#123;word&#125;</span>&#x27;的向量的前四个维度：<span class="hljs-subst">&#123;bert_emb2[: <span class="hljs-number">4</span>]&#125;</span>&quot;</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;在词向量模型中， &#x27;<span class="hljs-subst">&#123;word&#125;</span>&#x27; 的向量的前四个维度：<span class="hljs-subst">&#123;word_emb[: <span class="hljs-number">4</span>]&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">在句子 &#x27;我今天很开心。&#x27; 中，&#x27;开&#x27;的向量的前四个维度：[1.4325644  0.05137304 1.6045816  0.01002912]<br><br>在句子 &#x27;我打开了房门。&#x27; 中，&#x27;开&#x27;的向量的前四个维度：[ 0.9039772  -0.5877741   0.6639165   0.45880783]<br><br>在词向量模型中， &#x27;开&#x27; 的向量的前四个维度：[ 0.260962  0.040874  0.434256 -0.305888]<br></code></pre></td></tr></table></figure><p>BERT 模型果然能够根据上下文调整单词的向量。不妨再比较下余弦相似度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入用于计算余弦相似度的函数</span><br><span class="hljs-keyword">from</span> sklearn.metrics.pairwise <span class="hljs-keyword">import</span> cosine_similarity<br><br><span class="hljs-comment"># 计算两个BERT嵌入向量的余弦相似度</span><br>bert_similarity = cosine_similarity([bert_emb1], [bert_emb2])[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;在 &#x27;<span class="hljs-subst">&#123;sentence1&#125;</span>&#x27; 和 &#x27;<span class="hljs-subst">&#123;sentence2&#125;</span>&#x27; 这两个句子中，两个 &#x27;<span class="hljs-subst">&#123;word&#125;</span>&#x27; 的余弦相似度是: <span class="hljs-subst">&#123;bert_similarity:<span class="hljs-number">.2</span>f&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 计算词向量模型的两个向量之间的余弦相似度</span><br>word_similarity = cosine_similarity([word_emb], [word_emb])[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;在词向量中， &#x27;<span class="hljs-subst">&#123;word&#125;</span>&#x27; 和 &#x27;<span class="hljs-subst">&#123;word&#125;</span>&#x27; 的余弦相似度是: <span class="hljs-subst">&#123;word_similarity:<span class="hljs-number">.2</span>f&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>观察结果发现，不同句子中的“开”语义果然不同：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">在 &#x27;我今天很开心。&#x27; 和 &#x27;我打开了房门。&#x27; 这两个句子中，两个 &#x27;开&#x27; 的余弦相似度是: 0.69<br><br>在词向量中， &#x27;开&#x27; 和 &#x27;开&#x27; 的余弦相似度是: 1.00<br></code></pre></td></tr></table></figure><h2 id="怎么获得句子的向量"><a href="#怎么获得句子的向量" class="headerlink" title="怎么获得句子的向量"></a>怎么获得句子的向量</h2><p>我们虽然可以通过 BERT 模型获取单词的向量，但是怎么获得句子的向量呢？最简单的方法就是让 BERT 输出句子中每个单词的向量，然后计算向量的平均值。但是，这种不分重点一刀切的效果肯定是不好的，就好像我和千万富豪站在一起，计算我们的平均资产，然后得出结论，这两个人都是千万富翁，这显然不能反映真实情况。更关键的是，使用这种方法，并不能反映句子中词的顺序，而词序对句子语义的影响是非常大的。<br>&#96;&#96;<br>所以，想要反映句子的语义，必须使用专门的句子嵌入模型。它能够直接生成句子级别的嵌入表示，更好地捕捉句子中的上下文信息，从而生成更准确的句子向量。</p><p>句子嵌入模型是怎么训练的？一种常见方法是使用句子对。每次输入两个句子，分别生成它们的嵌入向量，计算相似度，然后与句子对自带的相似度做比较，通过差异调整嵌入模型的参数。</p><p>BGE_M3 模型就是这样一个嵌入模型，而且支持中文。</p><p>真的这么好用？是骡子是马，拉出来遛遛，我们比较一下这两种生成句子嵌入的方法。</p><p>首先，定义一个使用 BERT 模型获取句子向量的函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入 PyTorch 库</span><br><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-comment"># 使用 BERT 模型获取句子的向量</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_bert_sentence_emb</span>(<span class="hljs-params">sentence</span>):<br>    <span class="hljs-comment"># 使用 bert_tokenizer 对句子进行编码，得到 PyTorch 张量格式的输入</span><br>    <span class="hljs-built_in">input</span> = bert_tokenizer(sentence, return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>)<br>    <span class="hljs-comment"># print(f&quot;input: &#123;input&#125;&quot;)</span><br>    <span class="hljs-comment"># 将编码后的输入传递给 BERT 模型，计算所有层的输出</span><br>    output = bert_model(**<span class="hljs-built_in">input</span>)<br>    <span class="hljs-comment"># print(f&quot;output: &#123;output&#125;&quot;)</span><br>    <span class="hljs-comment"># 获取 BERT 模型最后一层的隐藏状态，它包含了每个单词的嵌入信息</span><br>    last_hidden_states = output.last_hidden_state<br>    <span class="hljs-comment"># 将所有词的向量求平均值，得到句子的表示</span><br>    sentence_emb = torch.mean(last_hidden_states, dim=<span class="hljs-number">1</span>).flatten().tolist()<br>    <span class="hljs-comment"># 返回句子的嵌入表示</span><br>    <span class="hljs-keyword">return</span> sentence_emb<br></code></pre></td></tr></table></figure><p>然后，定义一个用 bge_m3模型获取句子向量的函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入 bge_m3 模型</span><br><span class="hljs-keyword">from</span> pymilvus.model.hybrid <span class="hljs-keyword">import</span> BGEM3EmbeddingFunction<br><br><span class="hljs-comment"># 使用 bge_m3 模型获取句子的向量</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_bgem3_sentence_emb</span>(<span class="hljs-params">sentence, model_name=<span class="hljs-string">&#x27;BAAI/bge-m3&#x27;</span></span>):<br>    bge_m3_ef = BGEM3EmbeddingFunction(<br>        model_name=model_name,<br>        device=<span class="hljs-string">&#x27;cpu&#x27;</span>,<br>        use_fp16=<span class="hljs-literal">False</span><br>    )<br>    vectors = bge_m3_ef.encode_documents([sentence])<br>    <span class="hljs-keyword">return</span> vectors[<span class="hljs-string">&#x27;dense&#x27;</span>][<span class="hljs-number">0</span>].tolist()<br></code></pre></td></tr></table></figure><p>接下来，先计算下 BERT 模型生成的句子向量之间的余弦相似度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">sentence1 = <span class="hljs-string">&quot;我喜欢这部电影！&quot;</span><br>sentence2 = <span class="hljs-string">&quot;这部电影太棒了！&quot;</span><br>sentence3 = <span class="hljs-string">&quot;我讨厌这部电影。&quot;</span><br><br><span class="hljs-comment"># 使用 BERT 模型获取句子的向量</span><br>bert_sentence_emb1 = get_bert_sentence_emb(sentence1)<br>bert_sentence_emb2 = get_bert_sentence_emb(sentence2)<br>bert_sentence_emb3 = get_bert_sentence_emb(sentence3)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;<span class="hljs-subst">&#123;sentence1&#125;</span>&#x27; 和 &#x27;<span class="hljs-subst">&#123;sentence2&#125;</span>&#x27; 的余弦相似度: <span class="hljs-subst">&#123;cosine_similarity([bert_sentence_emb1], [bert_sentence_emb2])[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]:<span class="hljs-number">.2</span>f&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;<span class="hljs-subst">&#123;sentence1&#125;</span>&#x27; 和 &#x27;<span class="hljs-subst">&#123;sentence3&#125;</span>&#x27; 的余弦相似度: <span class="hljs-subst">&#123;cosine_similarity([bert_sentence_emb1], [bert_sentence_emb3])[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]:<span class="hljs-number">.2</span>f&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;<span class="hljs-subst">&#123;sentence2&#125;</span>&#x27; 和 &#x27;<span class="hljs-subst">&#123;sentence3&#125;</span>&#x27; 的余弦相似度: <span class="hljs-subst">&#123;cosine_similarity([bert_sentence_emb2], [bert_sentence_emb3])[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]:<span class="hljs-number">.2</span>f&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>结果是：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">&#x27;我喜欢这部电影！&#x27; 和 &#x27;这部电影太棒了！&#x27; 的余弦相似度: 0.93<br>&#x27;我喜欢这部电影！&#x27; 和 &#x27;我讨厌这部电影。&#x27; 的余弦相似度: 0.94<br>&#x27;这部电影太棒了！&#x27; 和 &#x27;我讨厌这部电影。&#x27; 的余弦相似度: 0.89<br></code></pre></td></tr></table></figure><p>很明显，前两个句子语义相近，并且与第三个句子语义相反。但是使用 BERT 模型的结果却是三个句子语义相近。</p><p>最后看看 bge_m3模型的效果如何：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用 bge_m3 模型获取句子的向量</span><br>bgem3_sentence_emb1 = get_bgem3_sentence_emb(sentence1)<br>bgem3_sentence_emb2 = get_bgem3_sentence_emb(sentence2)<br>bgem3_sentence_emb3 = get_bgem3_sentence_emb(sentence3)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;<span class="hljs-subst">&#123;sentence1&#125;</span>&#x27; 和 &#x27;<span class="hljs-subst">&#123;sentence2&#125;</span>&#x27; 的余弦相似度: <span class="hljs-subst">&#123;cosine_similarity([bgem3_sentence_emb1], [bgem3_sentence_emb2])[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]:<span class="hljs-number">.2</span>f&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;<span class="hljs-subst">&#123;sentence1&#125;</span>&#x27; 和 &#x27;<span class="hljs-subst">&#123;sentence3&#125;</span>&#x27; 的余弦相似度: <span class="hljs-subst">&#123;cosine_similarity([bgem3_sentence_emb1], [bgem3_sentence_emb3])[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]:<span class="hljs-number">.2</span>f&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;<span class="hljs-subst">&#123;sentence2&#125;</span>&#x27; 和 &#x27;<span class="hljs-subst">&#123;sentence3&#125;</span>&#x27; 的余弦相似度: <span class="hljs-subst">&#123;cosine_similarity([bgem3_sentence_emb2], [bgem3_sentence_emb3])[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]:<span class="hljs-number">.2</span>f&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>结果是：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">&#x27;我喜欢这部电影！&#x27; 和 &#x27;这部电影太棒了！&#x27; 的余弦相似度: 0.86<br>&#x27;我喜欢这部电影！&#x27; 和 &#x27;我讨厌这部电影。&#x27; 的余弦相似度: 0.65<br>&#x27;这部电影太棒了！&#x27; 和 &#x27;我讨厌这部电影。&#x27; 的余弦相似度: 0.57<br></code></pre></td></tr></table></figure><p>从余弦相似度可以看出，前两个句子语义相近，和第三个句子语义较远。看来 bge_m3 模型确实可以捕捉句子中的上下文信息。</p><h2 id="藏宝图"><a href="#藏宝图" class="headerlink" title="藏宝图"></a>藏宝图</h2><p>本文主要通过执行代码直观展示向量嵌入的原理和模型，如果你想进一步了解技术细节，这里有一些资料供你参考。</p><h3 id="词向量模型"><a href="#词向量模型" class="headerlink" title="词向量模型"></a>词向量模型</h3><p>word2vect 模型论文：</p><ul><li><a href="https://arxiv.org/abs/1301.3781">Efficient Estimation of Word Representations in Vector Space</a></li><li><a href="https://arxiv.org/abs/1310.4546">Distributed Representations of Words and Phrases and their Compositionality</a></li></ul><h3 id="中文词向量模型"><a href="#中文词向量模型" class="headerlink" title="中文词向量模型"></a>中文词向量模型</h3><ul><li><a href="https://github.com/Embedding/Chinese-Word-Vectors">Chinese-Word-Vectors</a> 项目提供了上百种预训练的中文词向量，这些词向量是基于不同的表征、上下文特征和语料库训练的，可以用于各种中文自然语言处理任务。</li><li><a href="https://ai.tencent.com/ailab/nlp/en/embedding.html">腾讯 AI Lab 中英文词和短语的嵌入语料库</a></li><li><a href="https://github.com/lzhenboy/word2vec-Chinese">word2vec-Chinese</a> 介绍了如何训练中文 Word2Vec 词向量模型。</li></ul><h3 id="BERT-模型"><a href="#BERT-模型" class="headerlink" title="BERT 模型"></a>BERT 模型</h3><p>BERT 模型论文：</p><ul><li><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li><li><a href="https://arxiv.org/abs/2004.12832">ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT</a></li></ul><p>BERT 模型的 GitHub：<a href="https://github.com/google-research/bert">bert</a></p><p>介绍 ColBERT 模型的博客：<a href="https://zilliz.com/learn/explore-colbert-token-level-embedding-and-ranking-model-for-similarity-search">Exploring ColBERT: A Token-Level Embedding and Ranking Model for Efficient Similarity Search</a></p><h3 id="bge-m3-模型"><a href="#bge-m3-模型" class="headerlink" title="bge_m3 模型"></a>bge_m3 模型</h3><p>介绍 bge_m3模型的博客：<a href="https://zilliz.com/learn/bge-m3-and-splade-two-machine-learning-models-for-generating-sparse-embeddings#BERT-The-Foundation-Model-for-BGE-M3-and-Splade">Exploring BGE-M3 and Splade: Two Machine Learning Models for Generating Sparse Embeddings</a></p><h3 id="注意力模型"><a href="#注意力模型" class="headerlink" title="注意力模型"></a>注意力模型</h3><p>注意力模型论文：<a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></p><h3 id="模型库"><a href="#模型库" class="headerlink" title="模型库"></a>模型库</h3><ul><li><a href="https://radimrehurek.com/gensim/">gensim</a> 包含了 word2vec 模型和 GloVe（Global Vectors for Word Representation）模型。</li><li><a href="https://huggingface.co/transformers/">Transformers</a> 是 Hugging Face 开发的一个开源库，专门用于自然语言处理（NLP）任务，它提供了大量预训练的 Transformer 模型，如 BERT、GPT、T5 等，并且支持多种语言和任务。</li><li><a href="https://github.com/ymcui/Chinese-BERT-wwm">Chinese-BERT-wwm</a> 是哈工大讯飞联合实验室（HFL）发布的中文 BERT 模型。</li><li><a href="https://milvus.io/docs/embeddings.md">pymilvus.model</a> 是 PyMilvus 客户端库的一个子包，提供多种嵌入模型的封装，用于生成向量嵌入，简化了文本转换过程。</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>严格来说，“目标词”不是单词而是“token”。token 是组成句子的基本单元。对于英文来说，token可以简单理解为单词，还可能是子词（subword）或者标点符号，比如“unhappiness” 可能会被分割成“un”和“happiness“。对于汉字来说，则是字、词或者短语，汉字不会像英文单词那样被分割。<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>向量数据库</category>
      
      <category>原理探秘</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>如何假装文艺青年，怎么把大白话“变成”古诗词？</title>
    <link href="/2024/09/16/%E5%A6%82%E4%BD%95%E5%81%87%E8%A3%85%E6%96%87%E8%89%BA%E9%9D%92%E5%B9%B4%EF%BC%8C%E6%80%8E%E4%B9%88%E6%8A%8A%E5%A4%A7%E7%99%BD%E8%AF%9D%E2%80%9C%E5%8F%98%E6%88%90%E2%80%9D%E5%8F%A4%E8%AF%97%E8%AF%8D%EF%BC%9F/"/>
    <url>/2024/09/16/%E5%A6%82%E4%BD%95%E5%81%87%E8%A3%85%E6%96%87%E8%89%BA%E9%9D%92%E5%B9%B4%EF%BC%8C%E6%80%8E%E4%B9%88%E6%8A%8A%E5%A4%A7%E7%99%BD%E8%AF%9D%E2%80%9C%E5%8F%98%E6%88%90%E2%80%9D%E5%8F%A4%E8%AF%97%E8%AF%8D%EF%BC%9F/</url>
    
    <content type="html"><![CDATA[<p>午后细雨绵绵，你独倚窗边，思绪万千，于是拿出手机，想发一条朋友圈抒发情怀，随便展示一下文采。奈何好不容易按出几个字，又全部删除。“今天的雨好大”展示不出你的文采。你灵机一动，如果有一个搜索引擎，能搜索出和“今天的雨好大”意思相近的古诗词，岂不妙哉！</p><p>使用向量数据库就可以实现，代码还不到100行，一起来试试吧。我们会从零开始安装向量数据库 Milvus，向量化古诗词数据集，然后创建集合，导入数据，创建索引，最后实现语义搜索功能。</p><p>本文首发于 Zilliz 公众号。文中代码的 Notebook 在<a href="https://pan.baidu.com/s/1Su0U65G6ZXXuzUy8VO3_cA?pwd=esca">这里</a>下载。</p><h2 id="0-准备工作"><a href="#0-准备工作" class="headerlink" title="0 准备工作"></a>0 准备工作</h2><p>首先安装向量数据库 Milvus。因为 Milvus 是运行在 docker 中的，所以需要先安装 Docker Desktop。MacOS 系统安装方法：<a href="https://docs.docker.com/desktop/install/mac-install/">Install Docker Desktop on Mac</a> ，Windows 系统安装方法：<a href="https://docs.docker.com/desktop/install/windows-install/">Install Docker Desktop on Windows</a></p><p>然后安装 Milvus。Milvus 版本：&gt;&#x3D;2.5.0<br>下载安装脚本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">curl -sfL https://raw.githubusercontent.com/milvus-io/milvus/master/scripts/standalone_embed.sh -o standalone_embed.sh<br></code></pre></td></tr></table></figure><p>运行 Milvus：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">bash standalone_embed.sh start<br></code></pre></td></tr></table></figure><p>安装依赖。pymilvus &gt;&#x3D; 2.5.0</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip install pymilvus &quot;pymilvus[model]&quot; torch <br></code></pre></td></tr></table></figure><p>下载古诗词数据集<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="古诗词数据集来自 [chinese-poetry](https://github.com/BushJiang/chinese-poetry)，数据结构做了调整。">[1]</span></a></sup> TangShi.json。它的格式是这样的：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">[</span><br><span class="hljs-punctuation">&#123;</span><br><span class="hljs-attr">&quot;author&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;太宗皇帝&quot;</span><span class="hljs-punctuation">,</span><br><span class="hljs-attr">&quot;paragraphs&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br><span class="hljs-string">&quot;秦川雄帝宅，函谷壮皇居。&quot;</span><br><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br><span class="hljs-attr">&quot;title&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;帝京篇十首 一&quot;</span><span class="hljs-punctuation">,</span><br><span class="hljs-attr">&quot;id&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">20000001</span><span class="hljs-punctuation">,</span><br><span class="hljs-attr">&quot;type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;唐诗&quot;</span><br><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>...<br><span class="hljs-punctuation">]</span><br></code></pre></td></tr></table></figure><p>准备就绪，正式开始啦。</p><h2 id="1-向量化文本"><a href="#1-向量化文本" class="headerlink" title="1 向量化文本"></a>1 向量化文本</h2><p>为了实现语义搜索，我们需要先把文本向量化。你可以理解为把不同类型的信息（如文字、图像、声音）翻译成计算机可以理解的数字语言。计算机理解了，才能帮你找到语义相近的诗句。</p><p>先定义两个函数，一个用来初始化嵌入模型（也就是用来向量化的模型）的实例，另一个是调用嵌入模型的实例，把输入的文档向量化。</p><p>初始化嵌入模型的实例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> pymilvus.model.hybrid <span class="hljs-keyword">import</span> BGEM3EmbeddingFunction<br><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">List</span>, <span class="hljs-type">Dict</span>, <span class="hljs-type">Union</span><br><span class="hljs-keyword">from</span> scipy.sparse <span class="hljs-keyword">import</span> csr_array<br><br><span class="hljs-comment"># 初始化嵌入模型的实例</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">init_embedding_model</span>() -&gt; BGEM3EmbeddingFunction:<br>    <span class="hljs-comment"># 检查是否有可用的CUDA设备</span><br>    device = <span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span><br>    <span class="hljs-comment"># 根据设备选择是否使用fp16</span><br>    use_fp16 = device.startswith(<span class="hljs-string">&quot;cuda&quot;</span>)<br>    <span class="hljs-comment"># 创建嵌入模型实例</span><br>    bge_m3_ef = BGEM3EmbeddingFunction(<br>        model_name=<span class="hljs-string">&quot;BAAI/bge-m3&quot;</span>,<br>        device=device,<br>        use_fp16=use_fp16<br>    )<br>    <span class="hljs-keyword">return</span> bge_m3_ef<br><br>bge_m3_ef = init_embedding_model()<br></code></pre></td></tr></table></figure><p>向量化文档：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 把文档向量化</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">vectorize_docs</span>(<span class="hljs-params">docs: <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]</span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Union</span>[<span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>]], <span class="hljs-type">List</span>[csr_array]]]:<br>    <span class="hljs-comment"># 验证 docs 是否为字符串列表</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(docs, <span class="hljs-built_in">list</span>) <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">all</span>(<span class="hljs-built_in">isinstance</span>(doc, <span class="hljs-built_in">str</span>) <span class="hljs-keyword">for</span> doc <span class="hljs-keyword">in</span> docs):<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;docs必须是字符串列表。&quot;</span>)<br>    <span class="hljs-keyword">if</span> bge_m3_ef <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;嵌入模型未初始化，请先调用 initialize_embedding_model 函数。&quot;</span>)<br>    <span class="hljs-comment"># 把输入的文本向量化</span><br>    <span class="hljs-keyword">return</span> bge_m3_ef.encode_documents(docs)<br></code></pre></td></tr></table></figure><p>准备好后，我们就可以向量化整个数据集了。首先读取数据集 TangShi.json 中的数据，把其中的 paragraphs 字段向量化，然后写入 TangShi_vector.json 文件。如果你是第一次使用 Milvus，运行下面的代码时还会安装必要的依赖。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 读取 json 文件，把paragraphs字段向量化</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;TangShi.json&quot;</span>, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> file:<br>data_list = json.load(file)<br>docs = [data[<span class="hljs-string">&#x27;paragraphs&#x27;</span>][<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> data_list]<br><br><span class="hljs-comment"># 向量化文本数据</span><br>vectors = vectorize_docs(docs)<br><br><span class="hljs-comment"># 将向量添加到原始文本中</span><br><span class="hljs-keyword">for</span> data, vector <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(data_list, vectors[<span class="hljs-string">&#x27;dense&#x27;</span>]):<br>    <span class="hljs-comment"># 将 NumPy 数组转换为 Python 的普通列表</span><br>data[<span class="hljs-string">&#x27;vector&#x27;</span>] = vector.tolist()<br><br><span class="hljs-comment"># 将更新后的文本内容写入新的json文件</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;TangShi_vector.json&quot;</span>, <span class="hljs-string">&#x27;w&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> outfile:<br>json.dump(data_list, outfile, ensure_ascii=<span class="hljs-literal">False</span>, indent=<span class="hljs-number">4</span>)<br></code></pre></td></tr></table></figure><p>如果一切顺利，你会得到 TangShi_vector.json 文件，它增加了 vector 字段，它的值是一个字符串列表，也就是“向量”。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">[</span><br><span class="hljs-punctuation">&#123;</span><br><span class="hljs-attr">&quot;author&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;太宗皇帝&quot;</span><span class="hljs-punctuation">,</span><br><span class="hljs-attr">&quot;paragraphs&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br><span class="hljs-string">&quot;秦川雄帝宅，函谷壮皇居。&quot;</span><br><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br><span class="hljs-attr">&quot;title&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;帝京篇十首 一&quot;</span><span class="hljs-punctuation">,</span><br><span class="hljs-attr">&quot;id&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">20000001</span><span class="hljs-punctuation">,</span><br><span class="hljs-attr">&quot;type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;唐诗&quot;</span><span class="hljs-punctuation">,</span><br><span class="hljs-attr">&quot;vector&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br><span class="hljs-number">0.005114779807627201</span><span class="hljs-punctuation">,</span><br><span class="hljs-number">0.033538609743118286</span><span class="hljs-punctuation">,</span><br><span class="hljs-number">0.020395483821630478</span><span class="hljs-punctuation">,</span><br>...<br><span class="hljs-punctuation">]</span><br><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br><span class="hljs-punctuation">&#123;</span><br><span class="hljs-attr">&quot;author&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;太宗皇帝&quot;</span><span class="hljs-punctuation">,</span><br><span class="hljs-attr">&quot;paragraphs&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br><span class="hljs-string">&quot;绮殿千寻起，离宫百雉余。&quot;</span><br><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br><span class="hljs-attr">&quot;title&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;帝京篇十首 一&quot;</span><span class="hljs-punctuation">,</span><br><span class="hljs-attr">&quot;id&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">20000002</span><span class="hljs-punctuation">,</span><br><span class="hljs-attr">&quot;type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;唐诗&quot;</span><span class="hljs-punctuation">,</span><br><span class="hljs-attr">&quot;vector&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br><span class="hljs-number">-0.06334448605775833</span><span class="hljs-punctuation">,</span><br><span class="hljs-number">0.0017451602034270763</span><span class="hljs-punctuation">,</span><br><span class="hljs-number">-0.0010646708542481065</span><span class="hljs-punctuation">,</span><br>...<br><span class="hljs-punctuation">]</span><br><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>...<br><span class="hljs-punctuation">]</span><br></code></pre></td></tr></table></figure><h2 id="2-创建集合"><a href="#2-创建集合" class="headerlink" title="2 创建集合"></a>2 创建集合</h2><p>接下来我们要把向量数据导入向量数据库。当然，我们得先在向量数据库中创建一个集合，用来容纳向量数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> pymilvus <span class="hljs-keyword">import</span> MilvusClient<br><span class="hljs-comment"># 连接向量数据库，创建client实例</span><br>client = MilvusClient(uri=<span class="hljs-string">&quot;http://localhost:19530&quot;</span>)<br><br><span class="hljs-comment"># 指定集合名称</span><br>collection_name = <span class="hljs-string">&quot;TangShi&quot;</span><br></code></pre></td></tr></table></figure><p>为了避免向量数据库中存在同名集合，产生干扰，创建集合前先删除同名集合。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 检查同名集合是否存在，如果存在则删除</span><br><span class="hljs-keyword">if</span> client.has_collection(collection_name):<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Collection <span class="hljs-subst">&#123;collection_name&#125;</span> already exists&quot;</span>)<br><span class="hljs-keyword">try</span>:<br><span class="hljs-comment"># 删除同名集合</span><br>client.drop_collection(collection_name)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Deleted the collection <span class="hljs-subst">&#123;collection_name&#125;</span>&quot;</span>)<br><span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Error occurred while dropping collection: <span class="hljs-subst">&#123;e&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>我们把数据填入 excel 表格前，需要先设计好表头，规定有哪些字段，各个字段的数据类型是怎样的。向量数据库也是一样，它的“表头”就是 schema，模式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> pymilvus <span class="hljs-keyword">import</span> DataType<br><br><span class="hljs-comment"># 创建集合模式</span><br>schema = MilvusClient.create_schema(<br>auto_id=<span class="hljs-literal">False</span>,<br>enable_dynamic_field=<span class="hljs-literal">True</span>,<br>description=<span class="hljs-string">&quot;TangShi&quot;</span><br>)<br><br><span class="hljs-comment"># 添加字段到schema</span><br>schema.add_field(field_name=<span class="hljs-string">&quot;id&quot;</span>, datatype=DataType.INT64, is_primary=<span class="hljs-literal">True</span>)<br>schema.add_field(field_name=<span class="hljs-string">&quot;vector&quot;</span>, datatype=DataType.FLOAT_VECTOR, dim=<span class="hljs-number">1024</span>)<br>schema.add_field(field_name=<span class="hljs-string">&quot;title&quot;</span>, datatype=DataType.VARCHAR, max_length=<span class="hljs-number">1024</span>)<br>schema.add_field(field_name=<span class="hljs-string">&quot;author&quot;</span>, datatype=DataType.VARCHAR, max_length=<span class="hljs-number">256</span>)<br>schema.add_field(field_name=<span class="hljs-string">&quot;paragraphs&quot;</span>, datatype=DataType.VARCHAR, max_length=<span class="hljs-number">10240</span>)<br>schema.add_field(field_name=<span class="hljs-string">&quot;type&quot;</span>, datatype=DataType.VARCHAR, max_length=<span class="hljs-number">128</span>)<br></code></pre></td></tr></table></figure><p>模式创建好了，接下来就可以创建集合了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 创建集合</span><br><span class="hljs-keyword">try</span>:<br>client.create_collection(<br>collection_name=collection_name,<br>schema=schema,<br>shards_num=<span class="hljs-number">2</span><br>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Created collection <span class="hljs-subst">&#123;collection_name&#125;</span>&quot;</span>)<br><span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Error occurred while creating collection: <span class="hljs-subst">&#123;e&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><h2 id="3-入库"><a href="#3-入库" class="headerlink" title="3 入库"></a>3 入库</h2><p>接下来把文件导入到 Milvus。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 读取和处理文件</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;TangShi_vector.json&quot;</span>, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> file:<br>data = json.load(file)<br><span class="hljs-comment"># paragraphs的值是列表，需要从列表中取出字符串，取代列表，以符合Milvus插入数据的要求</span><br><span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> data:<br>item[<span class="hljs-string">&quot;paragraphs&quot;</span>] = item[<span class="hljs-string">&quot;paragraphs&quot;</span>][<span class="hljs-number">0</span>]<br><br><span class="hljs-comment"># 将数据插入集合</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;正在将数据插入集合：<span class="hljs-subst">&#123;collection_name&#125;</span>&quot;</span>)<br>res = client.insert(<br>collection_name=collection_name,<br>data=data<br>)<br></code></pre></td></tr></table></figure><p>导入成功了吗？我们来验证下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;插入的实体数量: <span class="hljs-subst">&#123;res[<span class="hljs-string">&#x27;insert_count&#x27;</span>]&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>返回插入实体的数量，看来是成功了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">插入的实体数量: 4307<br></code></pre></td></tr></table></figure><h2 id="4-创建索引"><a href="#4-创建索引" class="headerlink" title="4 创建索引"></a>4 创建索引</h2><p>向量已经导入 Milvus，现在可以搜索了吗？别急，为了提高搜索效率，我们还需要创建索引。什么是索引？一些大部头图书的最后，一般都会有索引，它列出了书中出现的关键术语以及对应的页码，帮助你快速找到它们的位置。如果没有索引，那就只能用笨方法，从第一页开始一页一页往后找了。</p><p><img src="https://picgo233.oss-cn-hangzhou.aliyuncs.com/img/202408232208878.jpeg" alt="图片来源：自己拍的《英国皇家园艺学会植物繁育手册：用已有植物打造完美新植物》"><br>图片来源：自己拍的《英国皇家园艺学会植物繁育手册：用已有植物打造完美新植物》</p><p>Milvus 的索引也是如此。如果不创建索引，虽然也可以搜索，但是速度很慢，它会逐一比较查询向量与数据库中每一个向量，通过指定方法计算出两个向量之间的 <strong>距离</strong>，找出距离最近的几个向量。而创建索引之后，搜索速度会大大提升。</p><p>索引有不同的类型，适合不同的场景使用，我们以后会详细讨论这个问题。这里我们使用 IVF_FLAT。另外，计算<strong>距离</strong>的方法也有多种，我们使用 IP，也就是计算两个向量的内积。这些都是索引的参数，我们先创建这些参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 创建IndexParams对象，用于存储索引的各种参数</span><br>index_params = client.prepare_index_params()<br><span class="hljs-comment"># 设置索引名称</span><br>vector_index_name = <span class="hljs-string">&quot;vector_index&quot;</span><br><span class="hljs-comment"># 设置索引的各种参数</span><br>index_params.add_index(<br><span class="hljs-comment"># 指定为&quot;vector&quot;字段创建索引</span><br>field_name=<span class="hljs-string">&quot;vector&quot;</span>,<br><span class="hljs-comment"># 设置索引类型</span><br>index_type=<span class="hljs-string">&quot;IVF_FLAT&quot;</span>,<br><span class="hljs-comment"># 设置度量类型</span><br>metric_type=<span class="hljs-string">&quot;IP&quot;</span>,<br><span class="hljs-comment"># 设置索引聚类中心的数量</span><br>params=&#123;<span class="hljs-string">&quot;nlist&quot;</span>: <span class="hljs-number">128</span>&#125;,<br><span class="hljs-comment"># 指定索引名称</span><br>index_name=vector_index_name<br>)<br></code></pre></td></tr></table></figure><p>索引参数创建好了，现在终于可以创建索引了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;开始创建索引：<span class="hljs-subst">&#123;vector_index_name&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 创建索引</span><br>client.create_index(<br><span class="hljs-comment"># 指定为哪个集合创建索引</span><br>collection_name=collection_name,<br><span class="hljs-comment"># 使用前面创建的索引参数创建索引</span><br>index_params=index_params<br>)<br></code></pre></td></tr></table></figure><p>我们来验证下索引是否创建成功了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">indexes = client.list_indexes(<br>collection_name=collection_name<br>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;列出创建的索引：<span class="hljs-subst">&#123;indexes&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>返回了包含索引名称的列表，索引名称 vector_index 正是我们之前创建的。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">列出创建的索引：[&#x27;vector_index&#x27;]<br></code></pre></td></tr></table></figure><p>再来查看下索引的详情。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 查看索引详情</span><br>index_details = client.describe_index(<br>collection_name=collection_name,<br><span class="hljs-comment"># 指定索引名称，这里假设使用第一个索引</span><br>index_name=<span class="hljs-string">&quot;vector_index&quot;</span><br>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;索引vector_index详情：<span class="hljs-subst">&#123;index_details&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>返回了一个包含索引详细信息的字典，可以我们之前设置的索引参数，比如 nlist，index_type 和 metric_type 等等。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">索引vector_index详情：&#123;&#x27;nlist&#x27;: &#x27;128&#x27;, &#x27;index_type&#x27;: &#x27;IVF_FLAT&#x27;, &#x27;metric_type&#x27;: &#x27;IP&#x27;, &#x27;field_name&#x27;: &#x27;vector&#x27;, &#x27;index_name&#x27;: &#x27;vector_index&#x27;, &#x27;total_rows&#x27;: 0, &#x27;indexed_rows&#x27;: 0, &#x27;pending_index_rows&#x27;: 0, &#x27;state&#x27;: &#x27;Finished&#x27;&#125;<br></code></pre></td></tr></table></figure><h2 id="5-加载索引"><a href="#5-加载索引" class="headerlink" title="5 加载索引"></a>5 加载索引</h2><p>索引创建成功了，现在可以搜索了吗？等等，我们还需要把集合中的数据和索引，从硬盘加载到内存中。因为在内存中搜索更快。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;正在加载集合：<span class="hljs-subst">&#123;collection_name&#125;</span>&quot;</span>)<br>client.load_collection(collection_name=collection_name)<br></code></pre></td></tr></table></figure><p>加载完成了，仍然验证下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(client.get_load_state(collection_name=collection_name))<br></code></pre></td></tr></table></figure><p>返回加载状态 Loaded，没问题，加载完成。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">&#123;&#x27;state&#x27;: &lt;LoadState: Loaded&gt;&#125;<br></code></pre></td></tr></table></figure><h2 id="6-搜索"><a href="#6-搜索" class="headerlink" title="6 搜索"></a>6 搜索</h2><p>经过前面的一系列准备，现在我们终于可以回到开头的问题了，用现代白话文搜索语义相似的古诗词。</p><p>首先，把我们要搜索的现代白话文“翻译”成向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 获取查询向量</span><br>text = <span class="hljs-string">&quot;今天的雨好大&quot;</span><br>query_vectors = [vectorize_text([text])[<span class="hljs-string">&#x27;dense&#x27;</span>][<span class="hljs-number">0</span>].tolist()]<br></code></pre></td></tr></table></figure><p>然后，设置搜索参数，告诉 Milvus 怎么搜索。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 设置搜索参数</span><br>search_params = &#123;<br><span class="hljs-comment"># 设置度量类型</span><br><span class="hljs-string">&quot;metric_type&quot;</span>: <span class="hljs-string">&quot;IP&quot;</span>,<br><span class="hljs-comment"># 指定在搜索过程中要查询的聚类单元数量，增加nprobe值可以提高搜索精度，但会降低搜索速度</span><br><span class="hljs-string">&quot;params&quot;</span>: &#123;<span class="hljs-string">&quot;nprobe&quot;</span>: <span class="hljs-number">16</span>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>最后，我们还得告诉它怎么输出结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 指定搜索结果的数量，“limit=3”表示返回最相近的前3个搜索结果</span><br>limit = <span class="hljs-number">3</span><br><span class="hljs-comment"># 指定返回的字段</span><br>output_fields = [<span class="hljs-string">&quot;author&quot;</span>, <span class="hljs-string">&quot;title&quot;</span>, <span class="hljs-string">&quot;paragraphs&quot;</span>]<br></code></pre></td></tr></table></figure><p>一切就绪，让我们开始搜索吧！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">res1 = client.search(<br>collection_name=collection_name,<br><span class="hljs-comment"># 指定查询向量</span><br>data=query_vectors,<br><span class="hljs-comment"># 指定搜索的字段</span><br>anns_field=<span class="hljs-string">&quot;vector&quot;</span>,<br><span class="hljs-comment"># 设置搜索参数</span><br>search_params=search_params,<br><span class="hljs-comment"># 指定返回搜索结果的数量</span><br>limit=limit,<br><span class="hljs-comment"># 指定返回的字段</span><br>output_fields=output_fields<br>)<br><span class="hljs-built_in">print</span>(res1)<br></code></pre></td></tr></table></figure><p>得到下面的结果：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs shell">data: [<br>    &quot;[<br>        &#123;<br>            &#x27;id&#x27;: 20002740,<br>            &#x27;distance&#x27;: 0.6542239189147949,<br>            &#x27;entity&#x27;: &#123;<br>                &#x27;title&#x27;: &#x27;郊庙歌辞 享太庙乐章 大明舞&#x27;,<br>                &#x27;paragraphs&#x27;: &#x27;旱望春雨，云披大风。&#x27;,<br>                &#x27;author&#x27;: &#x27;张说&#x27;<br>            &#125;<br>        &#125;,<br>        &#123;<br>            &#x27;id&#x27;: 20001658,<br>            &#x27;distance&#x27;: 0.6228379011154175,<br>            &#x27;entity&#x27;: &#123;<br>                &#x27;title&#x27;: &#x27;三学山夜看圣灯&#x27;,<br>                &#x27;paragraphs&#x27;: &#x27;细雨湿不暗，好风吹更明。&#x27;,<br>                &#x27;author&#x27;: &#x27;蜀太妃徐氏&#x27;<br>            &#125;<br>        &#125;,<br>        &#123;<br>            &#x27;id&#x27;: 20003360,<br>            &#x27;distance&#x27;: 0.6123768091201782,<br>            &#x27;entity&#x27;: &#123;<br>                &#x27;title&#x27;: &#x27;郊庙歌辞 汉宗庙乐舞辞 积善舞&#x27;,<br>                &#x27;paragraphs&#x27;: &#x27;云行雨施，天成地平。&#x27;,<br>                &#x27;author&#x27;: &#x27;张昭&#x27;<br>            &#125;<br>        &#125;<br>    ]&quot;<br>]<br></code></pre></td></tr></table></figure><p>在搜索结果中，id、title 等字段我们都了解了，只有 distance 是新出现的。它指的是搜索结果与查询向量之间的“距离”，具体含义和度量类型有关。我们使用的度量类型是 IP 内积，数字越大表示搜索结果和查询向量越接近。</p><p>为了增加可读性，我们写一个输出函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 打印向量搜索结果</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">print_vector_results</span>(<span class="hljs-params">res</span>):<br>    <span class="hljs-comment"># hit是搜索结果中的每一个匹配的实体</span><br>    res = [hit[<span class="hljs-string">&quot;entity&quot;</span>] <span class="hljs-keyword">for</span> hit <span class="hljs-keyword">in</span> res[<span class="hljs-number">0</span>]]<br>    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> res:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;title: <span class="hljs-subst">&#123;item[<span class="hljs-string">&#x27;title&#x27;</span>]&#125;</span>&quot;</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;author: <span class="hljs-subst">&#123;item[<span class="hljs-string">&#x27;author&#x27;</span>]&#125;</span>&quot;</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;paragraphs: <span class="hljs-subst">&#123;item[<span class="hljs-string">&#x27;paragraphs&#x27;</span>]&#125;</span>&quot;</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;-&quot;</span>*<span class="hljs-number">50</span>)   <br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;数量：<span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(res)&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>重新输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">print_vector_results(res1)<br></code></pre></td></tr></table></figure><p>这下搜索结果容易阅读了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell">title: 郊庙歌辞 享太庙乐章 大明舞<br>author: 张说<br>paragraphs: 旱望春雨，云披大风。<br>--------------------------------------------------<br>title: 三学山夜看圣灯<br>author: 蜀太妃徐氏<br>paragraphs: 细雨湿不暗，好风吹更明。<br>--------------------------------------------------<br>title: 郊庙歌辞 汉宗庙乐舞辞 积善舞<br>author: 张昭<br>paragraphs: 云行雨施，天成地平。<br>--------------------------------------------------<br>数量：3<br></code></pre></td></tr></table></figure><p>如果你不想限制搜索结果的数量，而是返回所有质量符合要求的搜索结果，可以修改搜索参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 修改搜索参数，设置距离的范围</span><br>search_params = &#123;<br><span class="hljs-string">&quot;metric_type&quot;</span>: <span class="hljs-string">&quot;IP&quot;</span>,<br><span class="hljs-string">&quot;params&quot;</span>: &#123;<br><span class="hljs-string">&quot;nprobe&quot;</span>: <span class="hljs-number">16</span>,<br><span class="hljs-string">&quot;radius&quot;</span>: <span class="hljs-number">0.55</span>,<br><span class="hljs-string">&quot;range_filter&quot;</span>: <span class="hljs-number">1.0</span><br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>在搜索参数中增加 radius 和 range_filter 参数，它们限制了距离 distance 的范围在0.55到1之间。然后调整搜索代码，删除 limit 参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">res2 = client.search(<br>collection_name=collection_name,<br><span class="hljs-comment"># 指定查询向量</span><br>data=query_vectors,<br><span class="hljs-comment"># 指定搜索的字段</span><br>anns_field=<span class="hljs-string">&quot;vector&quot;</span>,<br><span class="hljs-comment"># 设置搜索参数</span><br>search_params=search_params,<br><span class="hljs-comment"># 删除limit参数</span><br><span class="hljs-comment"># 指定返回的字段</span><br>output_fields=output_fields<br>)<br><span class="hljs-built_in">print</span>(res2)<br></code></pre></td></tr></table></figure><p>可以看到，输出结果的 distance 都大于0.55。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python">data: [<br>    <span class="hljs-string">&quot;[</span><br><span class="hljs-string">        &#123;</span><br><span class="hljs-string">            &#x27;id&#x27;: 20002740,</span><br><span class="hljs-string">            &#x27;distance&#x27;: 0.6542239189147949,</span><br><span class="hljs-string">            &#x27;entity&#x27;: &#123;</span><br><span class="hljs-string">                &#x27;author&#x27;: &#x27;张说&#x27;,</span><br><span class="hljs-string">                &#x27;title&#x27;: &#x27;郊庙歌辞 享太庙乐章 大明舞&#x27;,</span><br><span class="hljs-string">                &#x27;paragraphs&#x27;: &#x27;旱望春雨，云披大风。&#x27;</span><br><span class="hljs-string">            &#125;</span><br><span class="hljs-string">        &#125;,</span><br><span class="hljs-string">        &#123;</span><br><span class="hljs-string">            &#x27;id&#x27;: 20001658,</span><br><span class="hljs-string">            &#x27;distance&#x27;: 0.6228379011154175,</span><br><span class="hljs-string">            &#x27;entity&#x27;: &#123;</span><br><span class="hljs-string">                &#x27;author&#x27;: &#x27;蜀太妃徐氏&#x27;,</span><br><span class="hljs-string">                &#x27;title&#x27;: &#x27;三学山夜看圣灯&#x27;,</span><br><span class="hljs-string">                &#x27;paragraphs&#x27;: &#x27;细雨湿不暗，好风吹更明。&#x27;</span><br><span class="hljs-string">            &#125;</span><br><span class="hljs-string">        &#125;,</span><br><span class="hljs-string">        &#123;</span><br><span class="hljs-string">            &#x27;id&#x27;: 20003360,</span><br><span class="hljs-string">            &#x27;distance&#x27;: 0.6123768091201782,</span><br><span class="hljs-string">            &#x27;entity&#x27;: &#123;</span><br><span class="hljs-string">                &#x27;author&#x27;: &#x27;张昭&#x27;,</span><br><span class="hljs-string">                &#x27;title&#x27;: &#x27;郊庙歌辞 汉宗庙乐舞辞 积善舞&#x27;,</span><br><span class="hljs-string">                &#x27;paragraphs&#x27;: &#x27;云行雨施，天成地平。&#x27;</span><br><span class="hljs-string">            &#125;</span><br><span class="hljs-string">        &#125;,</span><br><span class="hljs-string">        &#123;</span><br><span class="hljs-string">            &#x27;id&#x27;: 20003608,</span><br><span class="hljs-string">            &#x27;distance&#x27;: 0.5755923986434937,</span><br><span class="hljs-string">            &#x27;entity&#x27;: &#123;</span><br><span class="hljs-string">                &#x27;author&#x27;: &#x27;李端&#x27;,</span><br><span class="hljs-string">                &#x27;title&#x27;: &#x27;鼓吹曲辞 巫山高&#x27;,</span><br><span class="hljs-string">                &#x27;paragraphs&#x27;: &#x27;回合云藏日，霏微雨带风。&#x27;</span><br><span class="hljs-string">            &#125;</span><br><span class="hljs-string">        &#125;,</span><br><span class="hljs-string">        &#123;</span><br><span class="hljs-string">            &#x27;id&#x27;: 20000992,</span><br><span class="hljs-string">            &#x27;distance&#x27;: 0.5700664520263672,</span><br><span class="hljs-string">            &#x27;entity&#x27;: &#123;</span><br><span class="hljs-string">                &#x27;author&#x27;: &#x27;德宗皇帝&#x27;,</span><br><span class="hljs-string">                &#x27;title&#x27;: &#x27;九月十八赐百僚追赏因书所怀&#x27;,</span><br><span class="hljs-string">                &#x27;paragraphs&#x27;: &#x27;雨霁霜气肃，天高云日明。&#x27;</span><br><span class="hljs-string">            &#125;</span><br><span class="hljs-string">        &#125;,</span><br><span class="hljs-string">        &#123;</span><br><span class="hljs-string">            &#x27;id&#x27;: 20002246,</span><br><span class="hljs-string">            &#x27;distance&#x27;: 0.5583387613296509,</span><br><span class="hljs-string">            &#x27;entity&#x27;: &#123;</span><br><span class="hljs-string">                &#x27;author&#x27;: &#x27;不详&#x27;,</span><br><span class="hljs-string">                &#x27;title&#x27;: &#x27;郊庙歌辞 祭方丘乐章 顺和&#x27;,</span><br><span class="hljs-string">                &#x27;paragraphs&#x27;: &#x27;雨零感节，云飞应序。&#x27;</span><br><span class="hljs-string">            &#125;</span><br><span class="hljs-string">        &#125;</span><br><span class="hljs-string">    ]&quot;</span><br>]<br></code></pre></td></tr></table></figure><p>也许你还想知道你最喜欢的李白，有没有和你一样感慨今天的雨真大，没问题，我们增加filter参数就可以了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 通过表达式过滤字段author，筛选出字段“author”的值为“李白”的结果</span><br><span class="hljs-built_in">filter</span> = <span class="hljs-string">f&quot;author == &#x27;李白&#x27;&quot;</span><br><br>res3 = client.search(<br>collection_name=collection_name,<br><span class="hljs-comment"># 指定查询向量</span><br>data=query_vectors,<br><span class="hljs-comment"># 指定搜索的字段</span><br>anns_field=<span class="hljs-string">&quot;vector&quot;</span>,<br><span class="hljs-comment"># 设置搜索参数</span><br>search_params=search_params,<br><span class="hljs-comment"># 通过表达式实现标量过滤，筛选结果</span><br><span class="hljs-built_in">filter</span>=<span class="hljs-built_in">filter</span>,<br><span class="hljs-comment"># 指定返回搜索结果的数量</span><br>limit=limit,<br><span class="hljs-comment"># 指定返回的字段</span><br>output_fields=output_fields<br>)<br><span class="hljs-built_in">print</span>(res3)<br></code></pre></td></tr></table></figure><p>返回的结果为空值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">data: [<span class="hljs-string">&#x27;[]&#x27;</span>] <br></code></pre></td></tr></table></figure><p>这是因为我们前面设置了 distance 的范围在0.55到1之间，放大范围可以获得更多结果。把 “radius” 的值修改为0.2，再次运行命令，让我们看看李白是怎么感慨的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python">data: [<br>    <span class="hljs-string">&quot;[</span><br><span class="hljs-string">        &#123;</span><br><span class="hljs-string">            &#x27;id&#x27;: 20004246,</span><br><span class="hljs-string">            &#x27;distance&#x27;: 0.46472394466400146,</span><br><span class="hljs-string">            &#x27;entity&#x27;: &#123;</span><br><span class="hljs-string">                &#x27;author&#x27;: &#x27;李白&#x27;,</span><br><span class="hljs-string">                &#x27;title&#x27;: &#x27;横吹曲辞 关山月&#x27;,</span><br><span class="hljs-string">                &#x27;paragraphs&#x27;: &#x27;明月出天山，苍茫云海间。&#x27;</span><br><span class="hljs-string">            &#125;</span><br><span class="hljs-string">        &#125;,</span><br><span class="hljs-string">        &#123;</span><br><span class="hljs-string">            &#x27;id&#x27;: 20003707,</span><br><span class="hljs-string">            &#x27;distance&#x27;: 0.4347272515296936,</span><br><span class="hljs-string">            &#x27;entity&#x27;: &#123;</span><br><span class="hljs-string">                &#x27;author&#x27;: &#x27;李白&#x27;,</span><br><span class="hljs-string">                &#x27;title&#x27;: &#x27;鼓吹曲辞 有所思&#x27;,</span><br><span class="hljs-string">                &#x27;paragraphs&#x27;: &#x27;海寒多天风，白波连山倒蓬壶。&#x27;</span><br><span class="hljs-string">            &#125;</span><br><span class="hljs-string">        &#125;,</span><br><span class="hljs-string">        &#123;</span><br><span class="hljs-string">            &#x27;id&#x27;: 20003556,</span><br><span class="hljs-string">            &#x27;distance&#x27;: 0.40778297185897827,</span><br><span class="hljs-string">            &#x27;entity&#x27;: &#123;</span><br><span class="hljs-string">                &#x27;author&#x27;: &#x27;李白&#x27;,</span><br><span class="hljs-string">                &#x27;title&#x27;: &#x27;鼓吹曲辞 战城南&#x27;,</span><br><span class="hljs-string">                &#x27;paragraphs&#x27;: &#x27;去年战桑干源，今年战葱河道。&#x27;</span><br><span class="hljs-string">            &#125;</span><br><span class="hljs-string">        &#125;</span><br><span class="hljs-string">    ]&quot;</span><br>]<br></code></pre></td></tr></table></figure><p>我们观察搜索结果发现， distance 在0.4左右，小于之前设置的0.55，所以被排除了。另外，distance 数值较小，说明搜索结果并不是特别接近查询向量，而这几句诗词的确和“雨”的关系比较远。</p><p>如果你希望搜索结果中直接包含“雨”字，可以使用 query 方法做标量搜索。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># paragraphs字段包含“雨”字</span><br><span class="hljs-built_in">filter</span> = <span class="hljs-string">f&quot;paragraphs like &#x27;%雨%&#x27;&quot;</span><br><br>res4 = client.query(<br>collection_name=collection_name,<br><span class="hljs-built_in">filter</span>=<span class="hljs-built_in">filter</span>,<br>output_fields=output_fields,<br>limit=limit<br>)<br><span class="hljs-built_in">print</span>(res4)<br></code></pre></td></tr></table></figure><p>标量查询的代码更简单，因为它免去了和向量搜索相关的参数，比如查询向量 data，指定搜索字段的 anns_field 和搜索参数 search_params，搜索参数只有 filter 。</p><p>观察搜索结果发现，标量搜索结果的数据结构少了一个 “[]”，我们在提取具体字段时需要注意这一点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python">data: [<br>    <span class="hljs-string">&quot;&#123;</span><br><span class="hljs-string">        &quot;</span>autho<span class="hljs-string">r&quot;: &quot;</span>太宗皇帝<span class="hljs-string">&quot;,</span><br><span class="hljs-string">        &quot;</span>title<span class="hljs-string">&quot;: &quot;</span>咏雨<span class="hljs-string">&quot;,</span><br><span class="hljs-string">        &quot;</span>paragraphs<span class="hljs-string">&quot;: &quot;</span>罩云飘远岫，喷雨泛长河。<span class="hljs-string">&quot;,</span><br><span class="hljs-string">        &quot;</span><span class="hljs-built_in">id</span><span class="hljs-string">&quot;: 20000305</span><br><span class="hljs-string">    &#125;,</span><br><span class="hljs-string">    &#123;</span><br><span class="hljs-string">        &quot;</span>autho<span class="hljs-string">r&quot;: &quot;</span>太宗皇帝<span class="hljs-string">&quot;,</span><br><span class="hljs-string">        &quot;</span>title<span class="hljs-string">&quot;: &quot;</span>咏雨<span class="hljs-string">&quot;,</span><br><span class="hljs-string">        &quot;</span>paragraphs<span class="hljs-string">&quot;: &quot;</span>和气吹绿野，梅雨洒芳田。<span class="hljs-string">&quot;,</span><br><span class="hljs-string">        &quot;</span><span class="hljs-built_in">id</span><span class="hljs-string">&quot;: 20000402</span><br><span class="hljs-string">    &#125;,</span><br><span class="hljs-string">    &#123;</span><br><span class="hljs-string">        &quot;</span>autho<span class="hljs-string">r&quot;: &quot;</span>太宗皇帝<span class="hljs-string">&quot;,</span><br><span class="hljs-string">        &quot;</span>title<span class="hljs-string">&quot;: &quot;</span>赋得花庭雾<span class="hljs-string">&quot;,</span><br><span class="hljs-string">        &quot;</span>paragraphs<span class="hljs-string">&quot;: &quot;</span>还当杂行雨，髣髴隐遥空。<span class="hljs-string">&quot;,</span><br><span class="hljs-string">        &quot;</span><span class="hljs-built_in">id</span><span class="hljs-string">&quot;: 20000421</span><br><span class="hljs-string">    &#125;&quot;</span><br>]<br></code></pre></td></tr></table></figure><p>filter 表达式还有丰富的用法，比如同时搜索两个字段，author 字段指定为 “杜甫”，同时 paragraphs 字段仍然要求包含“雨”字：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">filter</span> = <span class="hljs-string">f&quot;author == &#x27;杜甫&#x27; &amp;&amp; paragraphs like &#x27;%雨%&#x27;&quot;</span><br><br>res5 = client.query(<br>collection_name=collection_name,<br><span class="hljs-built_in">filter</span>=<span class="hljs-built_in">filter</span>,<br>output_fields=output_fields,<br>limit=limit<br>)<br><span class="hljs-built_in">print</span>(res5)<br></code></pre></td></tr></table></figure><p>返回杜甫含有“雨”字的诗句：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">data: [<br><span class="hljs-string">&quot;&#123;</span><br><span class="hljs-string">&#x27;title&#x27;: &#x27;横吹曲辞 前出塞九首 七&#x27;, </span><br><span class="hljs-string">&#x27;paragraphs&#x27;: &#x27;驱马天雨雪，军行入高山。&#x27;, </span><br><span class="hljs-string">&#x27;id&#x27;: 20004039, </span><br><span class="hljs-string">&#x27;author&#x27;: &#x27;杜甫&#x27;</span><br><span class="hljs-string">&#125;&quot;</span><br>] <br></code></pre></td></tr></table></figure><p>更多标量搜索的表达式可以参考<a href="https://milvus.io/docs/get-and-scalar-query.md#Use-Basic-Operators">Get &amp; Scalar Query</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>可能这样的搜索结果并没有让你很满意，这里面有多个原因。首先，数据集太小了。只有4000多个句子，语义更接近的句子可能没有包含其中。其次，嵌入模型虽然支持中文，但是古诗词并不是它的专长。这就好像你找了个翻译帮你和老外交流，翻译虽然懂普通话，但是你满嘴四川方言，翻译也只能也蒙带猜，翻译质量可想而知。</p><p>如果你希望优化搜索功能，可以在 <a href="https://github.com/BushJiang/chinese-poetry">chinese-poetry</a> 下载完整的古诗词数据集，再找找专门用于古诗词的嵌入模型，相信搜索效果会有较大提升。</p><p>另外，我在以上代码的基础上，开发了一个命令行应用，有兴趣可以玩玩：<a href="https://github.com/BushJiang/searchPoems">语义搜索古诗词</a></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>古诗词数据集来自 <a href="https://github.com/BushJiang/chinese-poetry">chinese-poetry</a>，数据结构做了调整。<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>向量数据库</category>
      
      <category>原理探秘</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
