[{"title":"写给新读者的导航","url":"/2025/01/07/写给新读者的导航/","content":"\n你好啊朋友，我是江浩，一名AI大陆的探险者，目前主要关注向量数据库和大语言模型领域。\n\n在博客里，我会探秘AI神奇能力的背后原理。别担心，我会用有趣的言语和生动的类比来解释这些原理。你是否好奇，孙悟空 + 红楼梦 - 西游记 = ？那就来了解下向量嵌入吧。当你接触到向量嵌入后，你可能会问，既然已经有了稠密向量，为什么还需要稀疏向量？嗯，如果说稠密向量是领域专家，那么稀疏向量就是一个聪明的门外汉，面对领域外的知识，请教后者反而更合适。\n\n除了原理探秘，我还会用AI开发一些有趣的应用，比如，用白话文搜索语义相似的古诗词，让你体验一把“文艺青年”的乐趣。或者，开发一个“鲁迅说没有”的RAG应用，验证把所谓的“鲁迅名言”是否属实，它的原文又是怎样的。甚至，我还想和牛魔王对话，问问他更爱铁扇公主还是玉面狐狸。哈哈，有趣的想法太多，慢慢实现。\n\n对了，我在探索AI大陆时，也采集了不少鲜美的果实————优质资源，我会整理好了分享给你。\n\nAI大陆有趣又神奇，朋友，我邀请你和我同行。\n\nChangeLog\n2025-01-07","categories":["杂谈"]},{"title":"孙悟空 + 红楼梦 - 西游记 = ？向量嵌入之稠密向量","url":"/2024/10/11/孙悟空-红楼梦-西游记-？向量嵌入之稠密向量/","content":"\n一起来开个脑洞，如果孙悟空穿越到红楼梦的世界，他会成为谁？贾宝玉，林黛玉，还是薛宝钗？这看似一道文学题，但是我们不妨用数学方法来求解：`孙悟空 + 红楼梦 - 西游记 = ？`\n\n文字也能做运算？当然不行，但是把文字转换成数字之后，就可以用来计算了。而这个过程，叫做 “向量嵌入”。为什么要做向量嵌入？因为具有语义意义的数据，比如文本或者图像，人可以分辨相关程度，但是无法量化，更不能计算。比如，对于一组词“孙悟空、猪八戒、沙僧、西瓜、苹果、香蕉“，我会把“孙悟空、猪八戒、沙僧”分成一组，“西瓜、苹果、香蕉”分成另一组。但是，如果进一步提问，“孙悟空”是和“猪八戒”更相关，还是和“沙僧”更相关呢？这很难回答。\n\n而把这些信息转换成向量后，相关程度就可以通过它们在向量空间中的距离量化。甚至于，我们可以做 `孙悟空 + 红楼梦 - 西游记 = ？` 这样的脑洞数学题。\n\n本文首发于 Zilliz 公众号。文中代码的 Notebook 在[这里]( https://pan.baidu.com/s/1VtPt-6Y_hhxKn9uB4AMNFg?pwd=7zv9)下载。\n\n## 文字是怎么变成向量的\n怎么把文字变成向量呢？首先出现的是词向量，其中的代表是 word2vec 模型。它先准备一张词汇表，给每个词随机赋予一个向量，然后利用大量语料，通过 CBOW（Continuous Bag-of-Words）和 Skip-Gram 两种方法训练模型，不断优化字词的向量。\n\nCBOW 使用上下文（周围的词）预测目标词[^1]，而 Skip-Gram 则相反，通过目标词预测它的上下文。举个例子，对于“我爱吃冰淇淋”这个句子，CBOW方法已知上下文“我爱“和”冰淇淋”，计算出中间词的概率，比如，“吃”的概率是90%，“喝”的概率是7%，“玩”的概率是3%。然后再使用损失函数预测概率与实际概率的差异，最后通过反向传播算法，调整词向量模型的参数，使得损失函数最小化。训练词向量模型的最终目的，是捕捉词汇之间的语义关系，使得相关的词在向量空间中距离更近。\n\n打个比方，最初的词向量模型就像一个刚出生的孩子，对字词的理解是模糊的。父母在各种场景下和孩子说话，时不时考一考孩子，相当于用语料库训练模型。只不过训练模型的过程是不断迭代神经网络的参数，而教孩子说话，则是改变大脑皮层中神经元突触的连接。\n\n比如，父母会在吃饭前跟孩子说：\n“肚子饿了就要...”\n“要吃饭。”\n\n如果答错了，父母会纠正孩子：\n“吃饭之前要...”\n“要喝汤。”\n\"不对，吃饭之前要洗手。\"\n\n这就是在调整模型的参数。\n\n好了，纸上谈兵结束，咱们用代码实际操练一番吧。\n\n版本说明：\nMilvus 版本：>=2.5.0\npymilvus 版本：>=2.5.0\n\n安装依赖：\n```shell\npip install gensim scikit-learn transformers matplotlib\n```\n\n从 gensim.models 模块中导入 KeyedVectors 类，它用于存储和操作词向量。\n```python\nfrom gensim.models import KeyedVectors\n```\n\n在[这里](https://github.com/Embedding/Chinese-Word-Vectors/blob/master/README_zh.md)下载中文词向量模型 `Literature 文学作品`，并且加载该模型。\n\n```python\n# 加载中文词向量模型\nword_vectors = KeyedVectors.load_word2vec_format('sgns.literature.word', binary=False)\n```\n\n词向量模型其实就像一本字典。在字典里，每个字对应的是一条解释，在词向量模型中，每个词对应的是一个向量。\n\n我们使用的词向量模型是300维的，数量太多，可以只显示前4个维度的数值：\n```python\nprint(f\"'孙悟空'的向量的前四个维度：{word_vectors['孙悟空'].tolist()[:4]}\")\n```\n\n输出结果为：\n```plaintext\n'孙悟空'的向量的前四个维度：[-0.09262000024318695, -0.034056998789310455, -0.16306699812412262, -0.05771299824118614]\n```\n\n## 语义更近，距离更近\n前面我们提出了疑问，“孙悟空”是和“猪八戒”更相关，还是和“沙僧”更相关呢？在 [[01-如何假装文艺青年，怎么把大白话“变成”古诗词？]] 这篇文章中，我们使用内积 `IP` 计算两个向量的距离，这里我们使用余弦相似度来计算。\n\n```python\nprint(f\"'孙悟空'和'猪八戒'向量的余弦相似度是：{word_vectors.similarity('孙悟空', '猪八戒'):.2f}\")\n\nprint(f\"'孙悟空'和'沙僧'向量的余弦相似度是：{word_vectors.similarity('孙悟空', '沙僧'):.2f}\")\n```\n\n返回：\n```plaintext\n'孙悟空'和'猪八戒'向量的余弦相似度是：0.60\n'孙悟空'和'沙僧'向量的余弦相似度是：0.59\n```\n\n看来，孙悟空还是和猪八戒更相关。但是我们还不满足，我们还想知道，和孙悟空最相关的是谁。\n\n```python\n# 查找与“孙悟空”最相关的4个词\nsimilar_words = word_vectors.most_similar(\"孙悟空\", topn=4)\nprint(f\"与'孙悟空'最相关的4个词分别是：\")\nfor word, similarity in similar_words:\n\tprint(f\"{word}， 余弦相似度为：{similarity:.2f}\")\n```\n\n返回：\n```plaintext\n与'孙悟空'最相关的4个词分别是：\n悟空， 余弦相似度为：0.66\n唐僧， 余弦相似度为：0.61\n美猴王， 余弦相似度为：0.61\n猪八戒， 余弦相似度为：0.60\n```\n\n“孙悟空”和“悟空”、“美猴王”相关，这容易理解。为什么它还和“唐僧”、“猪八戒”相关呢？前面提到的词向量模型的训练原理解释，就是因为在训练文本中，“唐僧”、“猪八戒”经常出现在“孙悟空”这个词的上下文中。这不难理解——在《西游记》中，孙悟空经常救唐僧，还喜欢戏耍八戒。\n\n前面提到，训练词向量模型是为了让语义相关的词，在向量空间中距离更近。那么，我们可以测试一下，给出四组语义相近的词，考一考词向量模型，看它能否识别出来。\n\n第一组：西游记，三国演义，水浒传，红楼梦\n第二组：西瓜，苹果，香蕉，梨\n第三组：长江，黄河\n\n首先，获取这四组词的词向量：\n```python\n# 导入用于数值计算的库\nimport numpy as np\n\n# 定义要可视化的单词列表\nwords = [\"西游记\", \"三国演义\", \"水浒传\", \"红楼梦\", \n        \"西瓜\", \"苹果\", \"香蕉\", \"梨\", \n        \"长江\", \"黄河\"]\n\n# 使用列表推导式获取每个单词的向量\nvectors = np.array([word_vectors[word] for word in words])\n```\n\n然后，使用 PCA （Principal Component Analysis，组成分分析）把200维的向量降到2维，一个维度作为 x 坐标，另一个维度作为 y 坐标，这样就把高维向量投影到平面了，方便我们在二维图形上显示它们。换句话说，PCA 相当于《三体》中的二向箔，对高维向量实施了降维打击。\n\n```python\n# 导入用于降维的PCA类\nfrom sklearn.decomposition import PCA\n\n# 创建PCA对象，设置降至2维\npca = PCA(n_components=2)\n\n# 对词向量实施PCA降维\nvectors_pca = pca.fit_transform(vectors)\n```\n\n最后，在二维图形上显示降维后的向量。\n```python\n# 导入用于绘图的库\nimport matplotlib.pyplot as plt\n# 创建一个5x5英寸的图\nfig, axes = plt.subplots(1, 1, figsize=(7, 7))\n\n# 设置中文字体\nplt.rcParams['font.sans-serif'] = ['Heiti TC']\n# 确保负号能够正确显示\nplt.rcParams['axes.unicode_minus'] = False  \n\n# 使用PCA降维后的前两个维度作为x和y坐标绘制散点图\naxes.scatter(vectors_pca[:, 0], vectors_pca[:, 1])\n\n# 为每个点添加文本标注\nfor i, word in enumerate(words):\n    # 添加注释，设置文本内容、位置、样式等\n    # 要显示的文本（单词）\n    axes.annotate(word,\n                  # 点的坐标\n                  (vectors_pca[i, 0], vectors_pca[i, 1]),  \n                  # 文本相对于点的偏移量\n                  xytext=(2, 2),  \n                  # 指定偏移量的单位\n                  textcoords='offset points',  \n                  # 字体大小\n                  fontsize=10,  \n                  # 字体粗细\n                  fontweight='bold')  \n\n# 设置图表标题和字体大小\naxes.set_title('词向量', fontsize=14)\n\n# 自动调整子图参数，使之填充整个图像区域\nplt.tight_layout()\n\n# 在屏幕上显示图表\nplt.show()\n```\n\n从图中可以看出，同一组词的确在图中的距离更近。\n\n![600](https://picgo233.oss-cn-hangzhou.aliyuncs.com/img/202409161007773.png)\n\n既然可以把高维向量投影到二维，那么是不是也能投影到三维呢？当然可以，那样更酷。你可以在 [TensorFlow Embedding Projector](https://projector.tensorflow.org/) 上尝试下，输入单词，搜索与它最近的几个词，看看它们在三维空间上的位置关系。\n\n比如，输入 `apple`，最接近的5个词分别是 `OS`、`macintosh`、`amiga`、`ibm` 和 `microsoft`。\n\n![600](https://picgo233.oss-cn-hangzhou.aliyuncs.com/img/202409161023693.png)\n\n## 如果孙悟空穿越到红楼梦\n回到我们开篇的问题，把文本向量化后，就可以做运算了。如果孙悟空穿越到红楼梦，我们用下面的数学公式表示：\n`孙悟空 + 红楼梦 - 西游记`\n\n```python\nresult = word_vectors.most_similar(positive=[\"孙悟空\", \"红楼梦\"], negative=[\"西游记\"], topn=4)\n\nprint(f\"孙悟空 + 红楼梦 - 西游记 = {result}\")\n```\n\n答案为：\n```plaintext\n孙悟空 + 红楼梦 - 西游记 = [('唐僧', 0.4163001477718353), ('贾宝玉', 0.41606390476226807), ('妙玉', 0.39432790875434875), ('沙和尚', 0.3922004997730255)]\n```\n\n你是不是有点惊讶，因为答案中的“唐僧”和“沙和尚”根本就不是《红楼梦》中的人物。这是因为虽然词向量可以反映字词之间的语义相关性，但是它终究是在做数学题，不能像人类一样理解“孙悟空 + 红楼梦 - 西游记”背后的含义。答案中出现“唐僧”和“沙和尚”是因为它们和“孙悟空”更相关，而出现“贾宝玉”和“妙玉”则是因为它们和“红楼梦”更相关。\n\n不过，这样的测试还蛮有趣的，你也可以多尝试一下，有的结果还蛮符合直觉的。\n\n```python\nresult = word_vectors.most_similar(positive=[\"牛奶\", \"发酵\"], topn=1)\nprint(f\"牛奶 + 发酵 = {result[0][0]}\")\n\nresult = word_vectors.most_similar(positive=[\"男人\", \"泰国\"], topn=1)\nprint(f\"男人 + 泰国 = {result[0][0]}\")\n```\n\n计算的结果如下：\n```plaintext\n牛奶 + 发酵 = 变酸\n男人 + 泰国 = 女人\n```\n\n## 待优化！\n尝试把上面的计算题降维，显示在图像上，看看是否满足两个向量相加，等于第三个向量\n\n\n## 一词多义怎么办\n前面说过，词向量模型就像一本字典，每个词对应一个向量，而且是唯一一个向量。但是，在语言中一词多义的现象是非常常见的，比如对于 “苹果” 这个词，既可以指一种水果，也可以指一家电子产品公司。词向量模型在训练 “苹果”这个词的向量时，这两种语义都会考虑到，所以它在向量空间中将位于“水果”和 “电子产品公司”之间。这就好像你3月20号过生日，你同事3月30号过生日，你的领导为了给你们两个人一起过庆祝生日，选择了3月25号——不是任何一个人的生日。\n\n为了解决一词多义的问题，BERT（Bidirectional Encoder Representations from Transformers）模型诞生了。它是一种基于深度神经网络的预训练语言模型，使用 Transformer 架构，通过自注意力机制同时考虑一个 token 的前后上下文，并且根据上下文环境更新该 token 的向量。\n\n比如，“苹果”这个目标词的初始向量是从词库中获取的，向量的值是固定的。当注意力模型处理“苹果“这个词时，如果发现上下文中有“手机”一词，会给它分配更多权重，“苹果”的向量会更新，靠近“手机”的方向。如果上下文中有“水果”一词，则会靠近“水果”的方向。\n\n注意力模型分配权重是有策略的。它只会给上下文中与目标词关系紧密的词分配更多权重。所以，BERT 能够理解目标词与上下文之间的语义关系，根据上下文调整目标词的向量。\n\nBERT 的预训练分成两种训练方式。第一种训练方式叫做“掩码语言模型（Masked Language Model，MLM）”，和 word2vec 相似，它会随机选择句子中的一些词遮住，根据上下文信息预测这个词，再根据预测结果与真实结果的差异调整参数。第二种训练方式叫做“下一句预测（Next Sentence Prediction，NSP）”，每次输入两个句子，判断第二个句子是否是第一个句子的下一句，然后同样根据结果差异调整参数。\n\n说了这么多，BERT 模型的效果究竟怎么样？让我们动手试试吧。首先导入 BERT 模型，定义一个获取句子中指定单词的向量的函数。\n\n```python\n# 从transformers库中导入BertTokenizer类和BertModel类\nfrom transformers import BertTokenizer, BertModel\n\n# 加载分词器 BertTokenizer\nbert_tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n\n# 加载嵌入模型 BertModel\nbert_model = BertModel.from_pretrained('bert-base-chinese')\n\n# 使用BERT获取句子中指定单词的向量\ndef get_bert_emb(sentence, word):\n    # 使用 bert_tokenizer 对句子编码\n    input = bert_tokenizer(sentence, return_tensors='pt')\n    # 将编码传递给 BERT 模型，计算所有层的输出\n    output = bert_model(**input)\n    # 获取 BERT 模型最后一层的隐藏状态，它包含了每个单词的嵌入信息\n    last_hidden_states = output.last_hidden_state\n    # 将输入的句子拆分成单词，并生成一个列表\n    word_tokens = bert_tokenizer.tokenize(sentence)\n    # 获取目标单词在列表中的索引位置\n    word_index = word_tokens.index(word)\n    # 从最后一层隐藏状态中提取目标单词的嵌入表示\n    word_emb = last_hidden_states[0, word_index + 1, :]\n    # 返回目标单词的嵌入表示\n    return word_emb\n```\n\n然后通过 BERT 和词向量模型分别获取两个句子中指定单词的向量。\n\n```python\nsentence1 = \"我今天很开心。\"\nsentence2 = \"我打开了房门。\"\nword = \"开\"\n\n# 使用 BERT 模型获取句子中指定单词的向量\nbert_emb1 = get_bert_emb(sentence1, word).detach().numpy()\n\nbert_emb2 = get_bert_emb(sentence2, word).detach().numpy()\n\n# 使用词向量模型获取指定单词的向量\nword_emb = word_vectors[word]\n```\n\n最后，查看这三个向量的区别。\n\n```python\nprint(f\"在句子 '{sentence1}' 中，'{word}'的向量的前四个维度：{bert_emb1[: 4]}\")\n\nprint(f\"在句子 '{sentence2}' 中，'{word}'的向量的前四个维度：{bert_emb2[: 4]}\")\n\nprint(f\"在词向量模型中， '{word}' 的向量的前四个维度：{word_emb[: 4]}\")\n```\n\n结果为：\n```plaintext\n在句子 '我今天很开心。' 中，'开'的向量的前四个维度：[1.4325644  0.05137304 1.6045816  0.01002912]\n\n在句子 '我打开了房门。' 中，'开'的向量的前四个维度：[ 0.9039772  -0.5877741   0.6639165   0.45880783]\n\n在词向量模型中， '开' 的向量的前四个维度：[ 0.260962  0.040874  0.434256 -0.305888]\n```\n\nBERT 模型果然能够根据上下文调整单词的向量。不妨再比较下余弦相似度：\n\n```python\n# 导入用于计算余弦相似度的函数\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# 计算两个BERT嵌入向量的余弦相似度\nbert_similarity = cosine_similarity([bert_emb1], [bert_emb2])[0][0]\nprint(f\"在 '{sentence1}' 和 '{sentence2}' 这两个句子中，两个 '{word}' 的余弦相似度是: {bert_similarity:.2f}\")\n\n# 计算词向量模型的两个向量之间的余弦相似度\nword_similarity = cosine_similarity([word_emb], [word_emb])[0][0]\nprint(f\"在词向量中， '{word}' 和 '{word}' 的余弦相似度是: {word_similarity:.2f}\")\n```\n\n观察结果发现，不同句子中的“开”语义果然不同：\n```plaintext\n在 '我今天很开心。' 和 '我打开了房门。' 这两个句子中，两个 '开' 的余弦相似度是: 0.69\n\n在词向量中， '开' 和 '开' 的余弦相似度是: 1.00\n```\n\n## 怎么获得句子的向量\n我们虽然可以通过 BERT 模型获取单词的向量，但是怎么获得句子的向量呢？最简单的方法就是让 BERT 输出句子中每个单词的向量，然后计算向量的平均值。但是，这种不分重点一刀切的效果肯定是不好的，就好像我和千万富豪站在一起，计算我们的平均资产，然后得出结论，这两个人都是千万富翁，这显然不能反映真实情况。更关键的是，使用这种方法，并不能反映句子中词的顺序，而词序对句子语义的影响是非常大的。\n``\n所以，想要反映句子的语义，必须使用专门的句子嵌入模型。它能够直接生成句子级别的嵌入表示，更好地捕捉句子中的上下文信息，从而生成更准确的句子向量。\n\n句子嵌入模型是怎么训练的？一种常见方法是使用句子对。每次输入两个句子，分别生成它们的嵌入向量，计算相似度，然后与句子对自带的相似度做比较，通过差异调整嵌入模型的参数。\n\nBGE_M3 模型就是这样一个嵌入模型，而且支持中文。\n\n真的这么好用？是骡子是马，拉出来遛遛，我们比较一下这两种生成句子嵌入的方法。\n\n首先，定义一个使用 BERT 模型获取句子向量的函数。\n```python\n# 导入 PyTorch 库\nimport torch\n\n# 使用 BERT 模型获取句子的向量\ndef get_bert_sentence_emb(sentence):\n    # 使用 bert_tokenizer 对句子进行编码，得到 PyTorch 张量格式的输入\n    input = bert_tokenizer(sentence, return_tensors='pt')\n    # print(f\"input: {input}\")\n    # 将编码后的输入传递给 BERT 模型，计算所有层的输出\n    output = bert_model(**input)\n    # print(f\"output: {output}\")\n    # 获取 BERT 模型最后一层的隐藏状态，它包含了每个单词的嵌入信息\n    last_hidden_states = output.last_hidden_state\n    # 将所有词的向量求平均值，得到句子的表示\n    sentence_emb = torch.mean(last_hidden_states, dim=1).flatten().tolist()\n    # 返回句子的嵌入表示\n    return sentence_emb\n```\n\n然后，定义一个用 bge_m3模型获取句子向量的函数。\n```python\n# 导入 bge_m3 模型\nfrom pymilvus.model.hybrid import BGEM3EmbeddingFunction\n\n# 使用 bge_m3 模型获取句子的向量\ndef get_bgem3_sentence_emb(sentence, model_name='BAAI/bge-m3'):\n    bge_m3_ef = BGEM3EmbeddingFunction(\n        model_name=model_name,\n        device='cpu',\n        use_fp16=False\n    )\n    vectors = bge_m3_ef.encode_documents([sentence])\n    return vectors['dense'][0].tolist()\n```\n\n接下来，先计算下 BERT 模型生成的句子向量之间的余弦相似度。\n```python\nsentence1 = \"我喜欢这部电影！\"\nsentence2 = \"这部电影太棒了！\"\nsentence3 = \"我讨厌这部电影。\"\n\n# 使用 BERT 模型获取句子的向量\nbert_sentence_emb1 = get_bert_sentence_emb(sentence1)\nbert_sentence_emb2 = get_bert_sentence_emb(sentence2)\nbert_sentence_emb3 = get_bert_sentence_emb(sentence3)\n\nprint(f\"'{sentence1}' 和 '{sentence2}' 的余弦相似度: {cosine_similarity([bert_sentence_emb1], [bert_sentence_emb2])[0][0]:.2f}\")\nprint(f\"'{sentence1}' 和 '{sentence3}' 的余弦相似度: {cosine_similarity([bert_sentence_emb1], [bert_sentence_emb3])[0][0]:.2f}\")\nprint(f\"'{sentence2}' 和 '{sentence3}' 的余弦相似度: {cosine_similarity([bert_sentence_emb2], [bert_sentence_emb3])[0][0]:.2f}\")\n```\n\n结果是：\n```plaintext\n'我喜欢这部电影！' 和 '这部电影太棒了！' 的余弦相似度: 0.93\n'我喜欢这部电影！' 和 '我讨厌这部电影。' 的余弦相似度: 0.94\n'这部电影太棒了！' 和 '我讨厌这部电影。' 的余弦相似度: 0.89\n```\n\n很明显，前两个句子语义相近，并且与第三个句子语义相反。但是使用 BERT 模型的结果却是三个句子语义相近。\n\n最后看看 bge_m3模型的效果如何：\n```python\n# 使用 bge_m3 模型获取句子的向量\nbgem3_sentence_emb1 = get_bgem3_sentence_emb(sentence1)\nbgem3_sentence_emb2 = get_bgem3_sentence_emb(sentence2)\nbgem3_sentence_emb3 = get_bgem3_sentence_emb(sentence3)\n\nprint(f\"'{sentence1}' 和 '{sentence2}' 的余弦相似度: {cosine_similarity([bgem3_sentence_emb1], [bgem3_sentence_emb2])[0][0]:.2f}\")\nprint(f\"'{sentence1}' 和 '{sentence3}' 的余弦相似度: {cosine_similarity([bgem3_sentence_emb1], [bgem3_sentence_emb3])[0][0]:.2f}\")\nprint(f\"'{sentence2}' 和 '{sentence3}' 的余弦相似度: {cosine_similarity([bgem3_sentence_emb2], [bgem3_sentence_emb3])[0][0]:.2f}\")\n```\n\n结果是：\n```plaintext\n'我喜欢这部电影！' 和 '这部电影太棒了！' 的余弦相似度: 0.86\n'我喜欢这部电影！' 和 '我讨厌这部电影。' 的余弦相似度: 0.65\n'这部电影太棒了！' 和 '我讨厌这部电影。' 的余弦相似度: 0.57\n```\n\n从余弦相似度可以看出，前两个句子语义相近，和第三个句子语义较远。看来 bge_m3 模型确实可以捕捉句子中的上下文信息。\n\n## 藏宝图\n本文主要通过执行代码直观展示向量嵌入的原理和模型，如果你想进一步了解技术细节，这里有一些资料供你参考。\n\n### 词向量模型\nword2vect 模型论文：\n- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)\n- [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)\n\n### 中文词向量模型\n- [Chinese-Word-Vectors](https://github.com/Embedding/Chinese-Word-Vectors) 项目提供了上百种预训练的中文词向量，这些词向量是基于不同的表征、上下文特征和语料库训练的，可以用于各种中文自然语言处理任务。\n-  [腾讯 AI Lab 中英文词和短语的嵌入语料库](https://ai.tencent.com/ailab/nlp/en/embedding.html)\n-  [word2vec-Chinese](https://github.com/lzhenboy/word2vec-Chinese) 介绍了如何训练中文 Word2Vec 词向量模型。\n \n### BERT 模型\nBERT 模型论文：\n-  [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\n- [ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT](https://arxiv.org/abs/2004.12832)\n\nBERT 模型的 GitHub：[bert](https://github.com/google-research/bert)\n\n介绍 ColBERT 模型的博客：[Exploring ColBERT: A Token-Level Embedding and Ranking Model for Efficient Similarity Search](https://zilliz.com/learn/explore-colbert-token-level-embedding-and-ranking-model-for-similarity-search)\n\n### bge_m3 模型\n介绍 bge_m3模型的博客：[Exploring BGE-M3 and Splade: Two Machine Learning Models for Generating Sparse Embeddings](https://zilliz.com/learn/bge-m3-and-splade-two-machine-learning-models-for-generating-sparse-embeddings#BERT-The-Foundation-Model-for-BGE-M3-and-Splade)\n\n### 注意力模型\n注意力模型论文：[Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n\n### 模型库 \n-  [gensim](https://radimrehurek.com/gensim/) 包含了 word2vec 模型和 GloVe（Global Vectors for Word Representation）模型。\n- [Transformers](https://huggingface.co/transformers/) 是 Hugging Face 开发的一个开源库，专门用于自然语言处理（NLP）任务，它提供了大量预训练的 Transformer 模型，如 BERT、GPT、T5 等，并且支持多种语言和任务。\n- [Chinese-BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm) 是哈工大讯飞联合实验室（HFL）发布的中文 BERT 模型。\n- [pymilvus.model](https://milvus.io/docs/embeddings.md) 是 PyMilvus 客户端库的一个子包，提供多种嵌入模型的封装，用于生成向量嵌入，简化了文本转换过程。\n\n## 参考\n[^1]: 严格来说，“目标词”不是单词而是“token”。token 是组成句子的基本单元。对于英文来说，token可以简单理解为单词，还可能是子词（subword）或者标点符号，比如“unhappiness” 可能会被分割成“un”和“happiness“。对于汉字来说，则是字、词或者短语，汉字不会像英文单词那样被分割。","categories":["向量数据库","原理探秘"]},{"title":"如何假装文艺青年，怎么把大白话“变成”古诗词？","url":"/2024/09/16/如何假装文艺青年，怎么把大白话“变成”古诗词？/","content":"\n午后细雨绵绵，你独倚窗边，思绪万千，于是拿出手机，想发一条朋友圈抒发情怀，随便展示一下文采。奈何好不容易按出几个字，又全部删除。“今天的雨好大”展示不出你的文采。你灵机一动，如果有一个搜索引擎，能搜索出和“今天的雨好大”意思相近的古诗词，岂不妙哉！\n\n使用向量数据库就可以实现，代码还不到100行，一起来试试吧。我们会从零开始安装向量数据库 Milvus，向量化古诗词数据集，然后创建集合，导入数据，创建索引，最后实现语义搜索功能。\n\n本文首发于 Zilliz 公众号。文中代码的 Notebook 在[这里](https://pan.baidu.com/s/1Su0U65G6ZXXuzUy8VO3_cA?pwd=esca)下载。\n\n## 0 准备工作\n首先安装向量数据库 Milvus。因为 Milvus 是运行在 docker 中的，所以需要先安装 Docker Desktop。MacOS 系统安装方法：[Install Docker Desktop on Mac](https://docs.docker.com/desktop/install/mac-install/) ，Windows 系统安装方法：[Install Docker Desktop on Windows](https://docs.docker.com/desktop/install/windows-install/)\n\n然后安装 Milvus。Milvus 版本：>=2.5.0\n下载安装脚本：\n```shell\ncurl -sfL https://raw.githubusercontent.com/milvus-io/milvus/master/scripts/standalone_embed.sh -o standalone_embed.sh\n```\n\n运行 Milvus：\n```shell\nbash standalone_embed.sh start\n```\n\n安装依赖。pymilvus >= 2.5.0\n```shell\npip install pymilvus \"pymilvus[model]\" torch \n```\n\n下载古诗词数据集[^1] TangShi.json。它的格式是这样的：\n\n```json\n[\n\t{\n\t\t\"author\": \"太宗皇帝\",\n\t\t\"paragraphs\": [\n\t\t\t\"秦川雄帝宅，函谷壮皇居。\"\n\t\t],\n\t\t\"title\": \"帝京篇十首 一\",\n\t\t\"id\": 20000001,\n\t\t\"type\": \"唐诗\"\n\t},\n\t...\n]\n```\n\n准备就绪，正式开始啦。\n## 1 向量化文本\n为了实现语义搜索，我们需要先把文本向量化。你可以理解为把不同类型的信息（如文字、图像、声音）翻译成计算机可以理解的数字语言。计算机理解了，才能帮你找到语义相近的诗句。\n\n先定义两个函数，一个用来初始化嵌入模型（也就是用来向量化的模型）的实例，另一个是调用嵌入模型的实例，把输入的文档向量化。\n\n初始化嵌入模型的实例：\n```python\nfrom tqdm import tqdm\nimport torch\nfrom pymilvus.model.hybrid import BGEM3EmbeddingFunction\nfrom typing import List, Dict, Union\nfrom scipy.sparse import csr_array\n\n# 初始化嵌入模型的实例\ndef init_embedding_model() -> BGEM3EmbeddingFunction:\n    # 检查是否有可用的CUDA设备\n    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n    # 根据设备选择是否使用fp16\n    use_fp16 = device.startswith(\"cuda\")\n    # 创建嵌入模型实例\n    bge_m3_ef = BGEM3EmbeddingFunction(\n        model_name=\"BAAI/bge-m3\",\n        device=device,\n        use_fp16=use_fp16\n    )\n    return bge_m3_ef\n\nbge_m3_ef = init_embedding_model()\n```\n\n向量化文档：\n```python\n# 把文档向量化\ndef vectorize_docs(docs: List[str]) -> Dict[str, Union[List[List[float]], List[csr_array]]]:\n    # 验证 docs 是否为字符串列表\n    if not isinstance(docs, list) or not all(isinstance(doc, str) for doc in docs):\n        raise ValueError(\"docs必须是字符串列表。\")\n    if bge_m3_ef is None:\n        raise ValueError(\"嵌入模型未初始化，请先调用 initialize_embedding_model 函数。\")\n    # 把输入的文本向量化\n    return bge_m3_ef.encode_documents(docs)\n```\n\n准备好后，我们就可以向量化整个数据集了。首先读取数据集 TangShi.json 中的数据，把其中的 paragraphs 字段向量化，然后写入 TangShi_vector.json 文件。如果你是第一次使用 Milvus，运行下面的代码时还会安装必要的依赖。\n\n```python\n# 读取 json 文件，把paragraphs字段向量化\nwith open(\"TangShi.json\", 'r', encoding='utf-8') as file:\n\tdata_list = json.load(file)\n\tdocs = [data['paragraphs'][0] for data in data_list]\n\n# 向量化文本数据\nvectors = vectorize_docs(docs)\n\n# 将向量添加到原始文本中\nfor data, vector in zip(data_list, vectors['dense']):\n    # 将 NumPy 数组转换为 Python 的普通列表\n\tdata['vector'] = vector.tolist()\n\n# 将更新后的文本内容写入新的json文件\nwith open(\"TangShi_vector.json\", 'w', encoding='utf-8') as outfile:\n\tjson.dump(data_list, outfile, ensure_ascii=False, indent=4)\n```\n\n如果一切顺利，你会得到 TangShi_vector.json 文件，它增加了 vector 字段，它的值是一个字符串列表，也就是“向量”。\n\n```json\n[\n\t{\n\t\t\"author\": \"太宗皇帝\",\n\t\t\"paragraphs\": [\n\t\t\t\"秦川雄帝宅，函谷壮皇居。\"\n\t\t],\n\t\t\"title\": \"帝京篇十首 一\",\n\t\t\"id\": 20000001,\n\t\t\"type\": \"唐诗\",\n\t\t\"vector\": [\n\t\t\t0.005114779807627201,\n\t\t\t0.033538609743118286,\n\t\t\t0.020395483821630478,\n\t\t\t...\n\t\t]\n\t},\n\t{\n\t\t\"author\": \"太宗皇帝\",\n\t\t\"paragraphs\": [\n\t\t\t\"绮殿千寻起，离宫百雉余。\"\n\t\t],\n\t\t\"title\": \"帝京篇十首 一\",\n\t\t\"id\": 20000002,\n\t\t\"type\": \"唐诗\",\n\t\t\"vector\": [\n\t\t\t-0.06334448605775833,\n\t\t\t0.0017451602034270763,\n\t\t\t-0.0010646708542481065,\n\t\t\t...\n\t\t]\n\t},\n\t...\n]\n```\n\n## 2 创建集合\n接下来我们要把向量数据导入向量数据库。当然，我们得先在向量数据库中创建一个集合，用来容纳向量数据。\n```python\nfrom pymilvus import MilvusClient\n# 连接向量数据库，创建client实例\nclient = MilvusClient(uri=\"http://localhost:19530\")\n\n# 指定集合名称\ncollection_name = \"TangShi\"\n```\n\n为了避免向量数据库中存在同名集合，产生干扰，创建集合前先删除同名集合。\n```python\n# 检查同名集合是否存在，如果存在则删除\nif client.has_collection(collection_name):\n\tprint(f\"Collection {collection_name} already exists\")\n\ttry:\n\t\t# 删除同名集合\n\t\tclient.drop_collection(collection_name)\n\t\tprint(f\"Deleted the collection {collection_name}\")\n\texcept Exception as e:\n\t\tprint(f\"Error occurred while dropping collection: {e}\")\n```\n\n我们把数据填入 excel 表格前，需要先设计好表头，规定有哪些字段，各个字段的数据类型是怎样的。向量数据库也是一样，它的“表头”就是 schema，模式。\n```python\nfrom pymilvus import DataType\n\n# 创建集合模式\nschema = MilvusClient.create_schema(\n\tauto_id=False,\n\tenable_dynamic_field=True,\n\tdescription=\"TangShi\"\n)\n\n# 添加字段到schema\nschema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\nschema.add_field(field_name=\"vector\", datatype=DataType.FLOAT_VECTOR, dim=1024)\nschema.add_field(field_name=\"title\", datatype=DataType.VARCHAR, max_length=1024)\nschema.add_field(field_name=\"author\", datatype=DataType.VARCHAR, max_length=256)\nschema.add_field(field_name=\"paragraphs\", datatype=DataType.VARCHAR, max_length=10240)\nschema.add_field(field_name=\"type\", datatype=DataType.VARCHAR, max_length=128)\n```\n\n模式创建好了，接下来就可以创建集合了。\n```python\n# 创建集合\ntry:\n\tclient.create_collection(\n\t\tcollection_name=collection_name,\n\t\tschema=schema,\n\t\tshards_num=2\n\t)\n\tprint(f\"Created collection {collection_name}\")\nexcept Exception as e:\n\tprint(f\"Error occurred while creating collection: {e}\")\n```\n\n## 3 入库\n接下来把文件导入到 Milvus。\n```python\n# 读取和处理文件\nwith open(\"TangShi_vector.json\", 'r') as file:\n\tdata = json.load(file)\n\t# paragraphs的值是列表，需要从列表中取出字符串，取代列表，以符合Milvus插入数据的要求\n\tfor item in data:\n\t\titem[\"paragraphs\"] = item[\"paragraphs\"][0]\n\n# 将数据插入集合\nprint(f\"正在将数据插入集合：{collection_name}\")\nres = client.insert(\n\tcollection_name=collection_name,\n\tdata=data\n)\n```\n\n导入成功了吗？我们来验证下。\n```python\nprint(f\"插入的实体数量: {res['insert_count']}\")\n```\n\n返回插入实体的数量，看来是成功了。\n```shell\n插入的实体数量: 4307\n```\n\n## 4 创建索引\n向量已经导入 Milvus，现在可以搜索了吗？别急，为了提高搜索效率，我们还需要创建索引。什么是索引？一些大部头图书的最后，一般都会有索引，它列出了书中出现的关键术语以及对应的页码，帮助你快速找到它们的位置。如果没有索引，那就只能用笨方法，从第一页开始一页一页往后找了。\n\n![图片来源：自己拍的《英国皇家园艺学会植物繁育手册：用已有植物打造完美新植物》](https://picgo233.oss-cn-hangzhou.aliyuncs.com/img/202408232208878.jpeg)\n图片来源：自己拍的《英国皇家园艺学会植物繁育手册：用已有植物打造完美新植物》\n\nMilvus 的索引也是如此。如果不创建索引，虽然也可以搜索，但是速度很慢，它会逐一比较查询向量与数据库中每一个向量，通过指定方法计算出两个向量之间的 **距离**，找出距离最近的几个向量。而创建索引之后，搜索速度会大大提升。\n\n索引有不同的类型，适合不同的场景使用，我们以后会详细讨论这个问题。这里我们使用 IVF_FLAT。另外，计算**距离**的方法也有多种，我们使用 IP，也就是计算两个向量的内积。这些都是索引的参数，我们先创建这些参数。\n\n```python\n# 创建IndexParams对象，用于存储索引的各种参数\nindex_params = client.prepare_index_params()\n# 设置索引名称\nvector_index_name = \"vector_index\"\n# 设置索引的各种参数\nindex_params.add_index(\n\t# 指定为\"vector\"字段创建索引\n\tfield_name=\"vector\",\n\t# 设置索引类型\n\tindex_type=\"IVF_FLAT\",\n\t# 设置度量类型\n\tmetric_type=\"IP\",\n\t# 设置索引聚类中心的数量\n\tparams={\"nlist\": 128},\n\t# 指定索引名称\n\tindex_name=vector_index_name\n)\n```\n\n索引参数创建好了，现在终于可以创建索引了。\n```python\nprint(f\"开始创建索引：{vector_index_name}\")\n\n# 创建索引\nclient.create_index(\n\t# 指定为哪个集合创建索引\n\tcollection_name=collection_name,\n\t# 使用前面创建的索引参数创建索引\n\tindex_params=index_params\n)\n```\n\n我们来验证下索引是否创建成功了。\n```python\nindexes = client.list_indexes(\n\tcollection_name=collection_name\n)\nprint(f\"列出创建的索引：{indexes}\")\n```\n\n返回了包含索引名称的列表，索引名称 vector_index 正是我们之前创建的。\n```shell\n列出创建的索引：['vector_index']\n```\n  \n再来查看下索引的详情。\n```python\n# 查看索引详情\nindex_details = client.describe_index(\n\tcollection_name=collection_name,\n\t# 指定索引名称，这里假设使用第一个索引\n\tindex_name=\"vector_index\"\n)\nprint(f\"索引vector_index详情：{index_details}\")\n```\n\n返回了一个包含索引详细信息的字典，可以我们之前设置的索引参数，比如 nlist，index_type 和 metric_type 等等。\n```shell\n索引vector_index详情：{'nlist': '128', 'index_type': 'IVF_FLAT', 'metric_type': 'IP', 'field_name': 'vector', 'index_name': 'vector_index', 'total_rows': 0, 'indexed_rows': 0, 'pending_index_rows': 0, 'state': 'Finished'}\n```\n\n## 5 加载索引\n索引创建成功了，现在可以搜索了吗？等等，我们还需要把集合中的数据和索引，从硬盘加载到内存中。因为在内存中搜索更快。\n```python\nprint(f\"正在加载集合：{collection_name}\")\nclient.load_collection(collection_name=collection_name)\n```\n\n加载完成了，仍然验证下。\n```python\nprint(client.get_load_state(collection_name=collection_name))\n```\n\n返回加载状态 Loaded，没问题，加载完成。\n```shell\n{'state': <LoadState: Loaded>}\n```\n\n## 6 搜索\n经过前面的一系列准备，现在我们终于可以回到开头的问题了，用现代白话文搜索语义相似的古诗词。\n\n首先，把我们要搜索的现代白话文“翻译”成向量。\n```python\n# 获取查询向量\ntext = \"今天的雨好大\"\nquery_vectors = [vectorize_text([text])['dense'][0].tolist()]\n```\n\n然后，设置搜索参数，告诉 Milvus 怎么搜索。\n```python\n# 设置搜索参数\nsearch_params = {\n\t# 设置度量类型\n\t\"metric_type\": \"IP\",\n\t# 指定在搜索过程中要查询的聚类单元数量，增加nprobe值可以提高搜索精度，但会降低搜索速度\n\t\"params\": {\"nprobe\": 16}\n}\n```\n\n最后，我们还得告诉它怎么输出结果。\n```python\n# 指定搜索结果的数量，“limit=3”表示返回最相近的前3个搜索结果\nlimit = 3\n# 指定返回的字段\noutput_fields = [\"author\", \"title\", \"paragraphs\"]\n```\n\n一切就绪，让我们开始搜索吧！\n```python\nres1 = client.search(\n\tcollection_name=collection_name,\n\t# 指定查询向量\n\tdata=query_vectors,\n\t# 指定搜索的字段\n\tanns_field=\"vector\",\n\t# 设置搜索参数\n\tsearch_params=search_params,\n\t# 指定返回搜索结果的数量\n\tlimit=limit,\n\t# 指定返回的字段\n\toutput_fields=output_fields\n)\nprint(res1)\n```\n\n得到下面的结果：\n```shell\ndata: [\n    \"[\n        {\n            'id': 20002740,\n            'distance': 0.6542239189147949,\n            'entity': {\n                'title': '郊庙歌辞 享太庙乐章 大明舞',\n                'paragraphs': '旱望春雨，云披大风。',\n                'author': '张说'\n            }\n        },\n        {\n            'id': 20001658,\n            'distance': 0.6228379011154175,\n            'entity': {\n                'title': '三学山夜看圣灯',\n                'paragraphs': '细雨湿不暗，好风吹更明。',\n                'author': '蜀太妃徐氏'\n            }\n        },\n        {\n            'id': 20003360,\n            'distance': 0.6123768091201782,\n            'entity': {\n                'title': '郊庙歌辞 汉宗庙乐舞辞 积善舞',\n                'paragraphs': '云行雨施，天成地平。',\n                'author': '张昭'\n            }\n        }\n    ]\"\n]\n```\n\n在搜索结果中，id、title 等字段我们都了解了，只有 distance 是新出现的。它指的是搜索结果与查询向量之间的“距离”，具体含义和度量类型有关。我们使用的度量类型是 IP 内积，数字越大表示搜索结果和查询向量越接近。\n\n为了增加可读性，我们写一个输出函数：\n```python\n# 打印向量搜索结果\ndef print_vector_results(res):\n    # hit是搜索结果中的每一个匹配的实体\n    res = [hit[\"entity\"] for hit in res[0]]\n    for item in res:\n        print(f\"title: {item['title']}\")\n        print(f\"author: {item['author']}\")\n        print(f\"paragraphs: {item['paragraphs']}\")\n        print(\"-\"*50)   \n    print(f\"数量：{len(res)}\")\n```\n\n重新输出结果：\n```python\nprint_vector_results(res1)\n```\n\n这下搜索结果容易阅读了。\n```shell\ntitle: 郊庙歌辞 享太庙乐章 大明舞\nauthor: 张说\nparagraphs: 旱望春雨，云披大风。\n--------------------------------------------------\ntitle: 三学山夜看圣灯\nauthor: 蜀太妃徐氏\nparagraphs: 细雨湿不暗，好风吹更明。\n--------------------------------------------------\ntitle: 郊庙歌辞 汉宗庙乐舞辞 积善舞\nauthor: 张昭\nparagraphs: 云行雨施，天成地平。\n--------------------------------------------------\n数量：3\n```\n\n如果你不想限制搜索结果的数量，而是返回所有质量符合要求的搜索结果，可以修改搜索参数：\n```python\n# 修改搜索参数，设置距离的范围\nsearch_params = {\n\t\"metric_type\": \"IP\",\n\t\"params\": {\n\t\"nprobe\": 16,\n\t\"radius\": 0.55,\n\t\"range_filter\": 1.0\n\t}\n}\n```\n\n在搜索参数中增加 radius 和 range_filter 参数，它们限制了距离 distance 的范围在0.55到1之间。然后调整搜索代码，删除 limit 参数。\n```python\nres2 = client.search(\n\tcollection_name=collection_name,\n\t# 指定查询向量\n\tdata=query_vectors,\n\t# 指定搜索的字段\n\tanns_field=\"vector\",\n\t# 设置搜索参数\n\tsearch_params=search_params,\n\t# 删除limit参数\n\t# 指定返回的字段\n\toutput_fields=output_fields\n)\nprint(res2)\n```\n\n可以看到，输出结果的 distance 都大于0.55。\n```python\ndata: [\n    \"[\n        {\n            'id': 20002740,\n            'distance': 0.6542239189147949,\n            'entity': {\n                'author': '张说',\n                'title': '郊庙歌辞 享太庙乐章 大明舞',\n                'paragraphs': '旱望春雨，云披大风。'\n            }\n        },\n        {\n            'id': 20001658,\n            'distance': 0.6228379011154175,\n            'entity': {\n                'author': '蜀太妃徐氏',\n                'title': '三学山夜看圣灯',\n                'paragraphs': '细雨湿不暗，好风吹更明。'\n            }\n        },\n        {\n            'id': 20003360,\n            'distance': 0.6123768091201782,\n            'entity': {\n                'author': '张昭',\n                'title': '郊庙歌辞 汉宗庙乐舞辞 积善舞',\n                'paragraphs': '云行雨施，天成地平。'\n            }\n        },\n        {\n            'id': 20003608,\n            'distance': 0.5755923986434937,\n            'entity': {\n                'author': '李端',\n                'title': '鼓吹曲辞 巫山高',\n                'paragraphs': '回合云藏日，霏微雨带风。'\n            }\n        },\n        {\n            'id': 20000992,\n            'distance': 0.5700664520263672,\n            'entity': {\n                'author': '德宗皇帝',\n                'title': '九月十八赐百僚追赏因书所怀',\n                'paragraphs': '雨霁霜气肃，天高云日明。'\n            }\n        },\n        {\n            'id': 20002246,\n            'distance': 0.5583387613296509,\n            'entity': {\n                'author': '不详',\n                'title': '郊庙歌辞 祭方丘乐章 顺和',\n                'paragraphs': '雨零感节，云飞应序。'\n            }\n        }\n    ]\"\n]\n```\n\n也许你还想知道你最喜欢的李白，有没有和你一样感慨今天的雨真大，没问题，我们增加filter参数就可以了。\n```python\n# 通过表达式过滤字段author，筛选出字段“author”的值为“李白”的结果\nfilter = f\"author == '李白'\"\n\nres3 = client.search(\n\tcollection_name=collection_name,\n\t# 指定查询向量\n\tdata=query_vectors,\n\t# 指定搜索的字段\n\tanns_field=\"vector\",\n\t# 设置搜索参数\n\tsearch_params=search_params,\n\t# 通过表达式实现标量过滤，筛选结果\n\tfilter=filter,\n\t# 指定返回搜索结果的数量\n\tlimit=limit,\n\t# 指定返回的字段\n\toutput_fields=output_fields\n)\nprint(res3)\n```\n\n返回的结果为空值。\n```python\ndata: ['[]'] \n```\n\n这是因为我们前面设置了 distance 的范围在0.55到1之间，放大范围可以获得更多结果。把 \"radius\" 的值修改为0.2，再次运行命令，让我们看看李白是怎么感慨的。\n```python\ndata: [\n    \"[\n        {\n            'id': 20004246,\n            'distance': 0.46472394466400146,\n            'entity': {\n                'author': '李白',\n                'title': '横吹曲辞 关山月',\n                'paragraphs': '明月出天山，苍茫云海间。'\n            }\n        },\n        {\n            'id': 20003707,\n            'distance': 0.4347272515296936,\n            'entity': {\n                'author': '李白',\n                'title': '鼓吹曲辞 有所思',\n                'paragraphs': '海寒多天风，白波连山倒蓬壶。'\n            }\n        },\n        {\n            'id': 20003556,\n            'distance': 0.40778297185897827,\n            'entity': {\n                'author': '李白',\n                'title': '鼓吹曲辞 战城南',\n                'paragraphs': '去年战桑干源，今年战葱河道。'\n            }\n        }\n    ]\"\n]\n```\n\n我们观察搜索结果发现， distance 在0.4左右，小于之前设置的0.55，所以被排除了。另外，distance 数值较小，说明搜索结果并不是特别接近查询向量，而这几句诗词的确和“雨”的关系比较远。\n\n如果你希望搜索结果中直接包含“雨”字，可以使用 query 方法做标量搜索。\n```python\n# paragraphs字段包含“雨”字\nfilter = f\"paragraphs like '%雨%'\"\n\nres4 = client.query(\n\tcollection_name=collection_name,\n\tfilter=filter,\n\toutput_fields=output_fields,\n\tlimit=limit\n)\nprint(res4)\n```\n\n标量查询的代码更简单，因为它免去了和向量搜索相关的参数，比如查询向量 data，指定搜索字段的 anns_field 和搜索参数 search_params，搜索参数只有 filter 。\n\n观察搜索结果发现，标量搜索结果的数据结构少了一个 “[]”，我们在提取具体字段时需要注意这一点。\n```python\ndata: [\n    \"{\n        \"author\": \"太宗皇帝\",\n        \"title\": \"咏雨\",\n        \"paragraphs\": \"罩云飘远岫，喷雨泛长河。\",\n        \"id\": 20000305\n    },\n    {\n        \"author\": \"太宗皇帝\",\n        \"title\": \"咏雨\",\n        \"paragraphs\": \"和气吹绿野，梅雨洒芳田。\",\n        \"id\": 20000402\n    },\n    {\n        \"author\": \"太宗皇帝\",\n        \"title\": \"赋得花庭雾\",\n        \"paragraphs\": \"还当杂行雨，髣髴隐遥空。\",\n        \"id\": 20000421\n    }\"\n]\n```\n\nfilter 表达式还有丰富的用法，比如同时搜索两个字段，author 字段指定为 “杜甫”，同时 paragraphs 字段仍然要求包含“雨”字：\n```python\nfilter = f\"author == '杜甫' && paragraphs like '%雨%'\"\n\nres5 = client.query(\n\tcollection_name=collection_name,\n\tfilter=filter,\n\toutput_fields=output_fields,\n\tlimit=limit\n)\nprint(res5)\n```\n\n返回杜甫含有“雨”字的诗句：\n```python\ndata: [\n\t\"{\n\t\t'title': '横吹曲辞 前出塞九首 七', \n\t\t'paragraphs': '驱马天雨雪，军行入高山。', \n\t\t'id': 20004039, \n\t\t'author': '杜甫'\n\t}\"\n] \n```\n\n更多标量搜索的表达式可以参考[Get & Scalar Query](https://milvus.io/docs/get-and-scalar-query.md#Use-Basic-Operators)。\n\n## 总结\n可能这样的搜索结果并没有让你很满意，这里面有多个原因。首先，数据集太小了。只有4000多个句子，语义更接近的句子可能没有包含其中。其次，嵌入模型虽然支持中文，但是古诗词并不是它的专长。这就好像你找了个翻译帮你和老外交流，翻译虽然懂普通话，但是你满嘴四川方言，翻译也只能也蒙带猜，翻译质量可想而知。\n\n如果你希望优化搜索功能，可以在 [chinese-poetry](https://github.com/BushJiang/chinese-poetry) 下载完整的古诗词数据集，再找找专门用于古诗词的嵌入模型，相信搜索效果会有较大提升。\n\n另外，我在以上代码的基础上，开发了一个命令行应用，有兴趣可以玩玩：[语义搜索古诗词](https://github.com/BushJiang/searchPoems)\n\n## 参考\n[^1]:古诗词数据集来自 [chinese-poetry](https://github.com/BushJiang/chinese-poetry)，数据结构做了调整。","categories":["向量数据库","原理探秘"]}]